{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565752f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'this': 1, 'is': 2, 'food': 3, 'house': 4, 'garden': 5, 'loved': 6, 'the': 7, 'poorly': 8, 'disgusting': 9, 'made': 10, 'movie': 11, 'beautiful': 12, 'was': 13, 'absolutely': 14}\n",
      "Max sentence length:  4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# STEP 1: Data Setup and preprocessing\n",
    "# Toy dataset with sentiment classification\n",
    "sentences = [\n",
    "    (\"This food was disgusting\", 0),\n",
    "    (\"Garden is beautiful\", 1),\n",
    "    (\"House is poorly made\", 0),\n",
    "    (\"Absolutely loved the movie\", 1)\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "# 1. Get all unique words from the dataset\n",
    "words = set(word.lower() for sentence, _ in sentences for word in sentence.split())\n",
    "\n",
    "# 2. Create a word index dictionary starting with 1. 0 is for padding\n",
    "#  This is tokenization\n",
    "word2idx = {word: i + 1 for i, word in enumerate(words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# 3. Get number of words\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "# 4. Max length of sentences\n",
    "max_len = max(len(sentence.split()) for sentence, _ in sentences)\n",
    "\n",
    "print(\"Vocabulary: \", word2idx)\n",
    "print(\"Max sentence length: \", max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcce53d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example encoded + padded: [[5, 2, 12, 0], 1]\n"
     ]
    }
   ],
   "source": [
    "#  STEP 2: Text preprocessing - Encoding and padding\n",
    "# Encode sentence\n",
    "def encode_sentence(sentence):\n",
    "    #  Returns a list of tokens related to the words in the sentence in the order of the sentence\n",
    "    #  Essentially converting the sentence into a list of tokens\n",
    "    return [word2idx[word.lower()] for word in sentence.split()]\n",
    "\n",
    "# Pad the sentences to make sure all are the same size\n",
    "def pad_sequence(seq, max_len):\n",
    "    return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "encoded_data = []\n",
    "for sentence, label in sentences:\n",
    "    encoded = encode_sentence(sentence)\n",
    "    padded = pad_sequence(encoded, max_len)\n",
    "    encoded_data.append([padded, label])\n",
    "\n",
    "print(\"Example encoded + padded:\", encoded_data[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3802b87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  STEP 3: DataSet and Dataloader\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    \n",
    "dataset = SentimentDataset(encoded_data)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Self Attention Model\n",
    "class SelfAttentionSentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_words, embed_dim):\n",
    "        super().__init__() # We need to initialize base class so that pytorch can register submodules, and for layer tracking, .eval(), .train() etc.\n",
    "        self.embedding = nn.Embedding(num_words, embed_dim)\n",
    "        print(\"Embedding layer: \", self.embedding, \"\\n\")\n",
    "        self.q = nn.Linear(embed_dim, embed_dim) # parameters in_features, out_features, bias ( by default True)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, 2) # Binary Classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x) # (B, T, D) B = batch size( number of input sequences ), T = Tokens per input sequence, D = Dimensions of each token\n",
    "        print(\"Current embedding:\", embed, \"\\n\")\n",
    "        Q = self.q(embed) \n",
    "        K = self.k(embed)\n",
    "        V = self.v(embed)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5) # (B, T, T)\n",
    "        print(\"QKt / size of Q: \", scores, \"\\n\")\n",
    "        attn_weights = f.softmax(scores, dim=-1)\n",
    "        print(\"SOFTmax: \", attn_weights, \"\\n\")\n",
    "        attention_scores = torch.matmul(attn_weights, V)\n",
    "        print(\"*V: \", attention_scores, \"\\n\")\n",
    "\n",
    "        pooled = attention_scores.mean(dim=1)\n",
    "        print(\"Mean: \", pooled, \"\\n\")\n",
    "        out = self.fc(pooled)\n",
    "        print(\"FC: \", out, \"\\n\")\n",
    "        return out, attn_weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9624c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding layer:  Embedding(15, 3) \n",
      "\n",
      "Current embedding: tensor([[[ 0.1744,  0.2006,  2.1014],\n",
      "         [ 0.6646,  0.9013, -1.9234],\n",
      "         [-2.0484, -1.6120, -0.0655],\n",
      "         [-0.0544,  1.1288, -0.0716]],\n",
      "\n",
      "        [[ 1.8374, -0.3266,  0.2589],\n",
      "         [-0.2603,  0.1526,  0.1830],\n",
      "         [-1.2554, -0.1684, -0.0191],\n",
      "         [-0.1580, -0.4169,  1.4129]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.1115,  0.2679,  0.5557, -0.0397],\n",
      "         [-0.1164,  0.2011,  0.2139, -0.0555],\n",
      "         [-1.1781,  1.4798, -0.0876, -0.4793],\n",
      "         [-0.2228,  0.3296,  0.3783,  0.0048]],\n",
      "\n",
      "        [[ 0.3101,  0.1345,  0.1605,  0.3568],\n",
      "         [ 0.5559,  0.0461,  0.0061, -0.0844],\n",
      "         [ 0.9117, -0.0293, -0.1766, -0.3175],\n",
      "         [ 0.6600,  0.0719,  0.0276, -0.0100]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.1823, 0.2665, 0.3553, 0.1959],\n",
      "         [0.2071, 0.2845, 0.2882, 0.2201],\n",
      "         [0.0494, 0.7044, 0.1469, 0.0993],\n",
      "         [0.1719, 0.2987, 0.3136, 0.2158]],\n",
      "\n",
      "        [[0.2668, 0.2239, 0.2297, 0.2796],\n",
      "         [0.3697, 0.2221, 0.2133, 0.1949],\n",
      "         [0.4952, 0.1932, 0.1668, 0.1448],\n",
      "         [0.3848, 0.2137, 0.2045, 0.1969]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.0327,  0.1937, -0.6327],\n",
      "         [ 0.1081,  0.1799, -0.6786],\n",
      "         [ 0.3774,  0.5152, -1.1422],\n",
      "         [ 0.0905,  0.2112, -0.6967]],\n",
      "\n",
      "        [[ 0.0719,  0.1226, -0.3122],\n",
      "         [ 0.1133,  0.1860, -0.3182],\n",
      "         [ 0.1598,  0.2517, -0.2918],\n",
      "         [ 0.1181,  0.1921, -0.3099]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.1522,  0.2750, -0.7876],\n",
      "        [ 0.1158,  0.1881, -0.3080]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2318, -0.3320],\n",
      "        [-0.1029, -0.2742]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.6120,  0.6288,  0.4425],\n",
      "         [-0.2703,  0.1426,  0.1930],\n",
      "         [ 1.5183,  0.2956,  0.6049],\n",
      "         [-1.7729,  0.5209,  0.0219]],\n",
      "\n",
      "        [[ 0.2047,  0.3990,  0.2444],\n",
      "         [ 1.3678, -0.3552,  0.0337],\n",
      "         [-1.8579, -0.0303,  0.8093],\n",
      "         [ 0.2905,  0.4514, -0.8188]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.1783,  0.0435,  0.1305, -0.1898],\n",
      "         [-0.2369,  0.0311,  0.2365, -0.3274],\n",
      "         [ 0.1420,  0.1615,  0.0435,  0.2072],\n",
      "         [-0.4380, -0.0477,  0.2670, -0.5612]],\n",
      "\n",
      "        [[ 0.0550,  0.3479, -0.1301,  0.2195],\n",
      "         [ 0.0839,  0.3987, -0.0316,  0.0918],\n",
      "         [ 0.0083,  0.8652, -0.9272,  0.6004],\n",
      "         [ 0.0403,  0.3955, -0.2475,  0.2214]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2174, 0.2715, 0.2961, 0.2150],\n",
      "         [0.2072, 0.2709, 0.3327, 0.1893],\n",
      "         [0.2504, 0.2554, 0.2269, 0.2673],\n",
      "         [0.1857, 0.2743, 0.3758, 0.1642]],\n",
      "\n",
      "        [[0.2299, 0.3081, 0.1910, 0.2710],\n",
      "         [0.2343, 0.3209, 0.2087, 0.2361],\n",
      "         [0.1800, 0.4240, 0.0706, 0.3254],\n",
      "         [0.2286, 0.3261, 0.1714, 0.2740]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.2386,  0.0258, -0.6505],\n",
      "         [ 0.2516,  0.0387, -0.6290],\n",
      "         [ 0.2151, -0.0027, -0.6950],\n",
      "         [ 0.2655,  0.0547, -0.6040]],\n",
      "\n",
      "        [[ 0.2122,  0.2340, -0.5924],\n",
      "         [ 0.2001,  0.2202, -0.5732],\n",
      "         [ 0.2680,  0.3462, -0.5795],\n",
      "         [ 0.2209,  0.2500, -0.5887]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.2427,  0.0291, -0.6446],\n",
      "        [ 0.2253,  0.2626, -0.5835]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2253, -0.2986],\n",
      "        [-0.2196, -0.2570]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 1 - Loss: 1.3804\n",
      "Current embedding: tensor([[[ 0.1577,  0.2173,  2.0847],\n",
      "         [ 0.6813,  0.9180, -1.9401],\n",
      "         [-2.0651, -1.5953, -0.0822],\n",
      "         [-0.0377,  1.1455, -0.0883]],\n",
      "\n",
      "        [[ 0.1973,  0.3916,  0.2518],\n",
      "         [ 1.3603, -0.3626,  0.0411],\n",
      "         [-1.8653, -0.0377,  0.8167],\n",
      "         [ 0.2831,  0.4440, -0.8114]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-1.0531e-01,  2.4472e-01,  4.6482e-01, -2.6752e-02],\n",
      "         [-1.2407e-01,  2.0742e-01,  1.5200e-01, -5.3446e-02],\n",
      "         [-1.3026e+00,  1.6656e+00, -2.6316e-01, -4.3005e-01],\n",
      "         [-2.2162e-01,  3.1451e-01,  2.8519e-01,  9.5238e-03]],\n",
      "\n",
      "        [[ 5.1589e-02,  3.4339e-01, -1.4773e-01,  2.1570e-01],\n",
      "         [ 8.5537e-02,  3.9259e-01, -2.6943e-02,  8.9321e-02],\n",
      "         [ 3.2673e-04,  8.7156e-01, -9.8576e-01,  6.0664e-01],\n",
      "         [ 3.7782e-02,  3.9520e-01, -2.6645e-01,  2.2091e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.1898, 0.2693, 0.3356, 0.2053],\n",
      "         [0.2090, 0.2912, 0.2755, 0.2243],\n",
      "         [0.0389, 0.7577, 0.1101, 0.0932],\n",
      "         [0.1776, 0.3037, 0.2949, 0.2238]],\n",
      "\n",
      "        [[0.2306, 0.3087, 0.1889, 0.2717],\n",
      "         [0.2349, 0.3194, 0.2099, 0.2358],\n",
      "         [0.1787, 0.4270, 0.0667, 0.3276],\n",
      "         [0.2289, 0.3273, 0.1689, 0.2749]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.0910,  0.2200, -0.6883],\n",
      "         [ 0.1604,  0.2133, -0.7346],\n",
      "         [ 0.4836,  0.5967, -1.2636],\n",
      "         [ 0.1500,  0.2417, -0.7551]],\n",
      "\n",
      "        [[ 0.2174,  0.2416, -0.5964],\n",
      "         [ 0.2038,  0.2253, -0.5774],\n",
      "         [ 0.2704,  0.3522, -0.5793],\n",
      "         [ 0.2257,  0.2574, -0.5919]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.2213,  0.3179, -0.8604],\n",
      "        [ 0.2293,  0.2691, -0.5863]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.3084, -0.2769],\n",
      "        [-0.2365, -0.2403]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.8155, -0.3485,  0.2807],\n",
      "         [-0.2658,  0.1491,  0.1887],\n",
      "         [-1.2773, -0.1903,  0.0028],\n",
      "         [-0.1799, -0.4387,  1.4348]],\n",
      "\n",
      "        [[-0.5988,  0.6420,  0.4293],\n",
      "         [-0.2658,  0.1491,  0.1887],\n",
      "         [ 1.5315,  0.3087,  0.5917],\n",
      "         [-1.7597,  0.5341,  0.0087]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.2919,  0.1363,  0.1654,  0.3748],\n",
      "         [ 0.5586,  0.0138, -0.0675, -0.1540],\n",
      "         [ 0.9540, -0.0737, -0.2900, -0.4430],\n",
      "         [ 0.6550,  0.0417, -0.0413, -0.0779]],\n",
      "\n",
      "        [[-0.1903,  0.0198,  0.1416, -0.2231],\n",
      "         [-0.2453,  0.0138,  0.2499, -0.3537],\n",
      "         [ 0.1541,  0.1545,  0.0234,  0.2222],\n",
      "         [-0.4618, -0.0771,  0.2996, -0.6189]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2615, 0.2239, 0.2305, 0.2841],\n",
      "         [0.3839, 0.2226, 0.2053, 0.1882],\n",
      "         [0.5281, 0.1890, 0.1522, 0.1306],\n",
      "         [0.3968, 0.2149, 0.1977, 0.1906]],\n",
      "\n",
      "        [[0.2176, 0.2685, 0.3033, 0.2106],\n",
      "         [0.2069, 0.2681, 0.3395, 0.1856],\n",
      "         [0.2533, 0.2534, 0.2222, 0.2711],\n",
      "         [0.1830, 0.2688, 0.3918, 0.1564]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.0677,  0.1222, -0.3085],\n",
      "         [ 0.1114,  0.1905, -0.3063],\n",
      "         [ 0.1543,  0.2559, -0.2654],\n",
      "         [ 0.1139,  0.1943, -0.2974]],\n",
      "\n",
      "        [[ 0.2696,  0.0530, -0.6770],\n",
      "         [ 0.2800,  0.0635, -0.6534],\n",
      "         [ 0.2475,  0.0263, -0.7341],\n",
      "         [ 0.2936,  0.0788, -0.6206]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.1118,  0.1907, -0.2944],\n",
      "        [ 0.2727,  0.0554, -0.6713]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.1357, -0.2358],\n",
      "        [-0.2711, -0.2594]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 2 - Loss: 1.3502\n",
      "Current embedding: tensor([[[ 1.4416e-01,  2.3221e-01,  2.0697e+00],\n",
      "         [ 6.9593e-01,  9.3266e-01, -1.9550e+00],\n",
      "         [-2.0796e+00, -1.5805e+00, -9.3937e-02],\n",
      "         [-4.1281e-02,  1.1605e+00, -1.0330e-01]],\n",
      "\n",
      "        [[-5.9150e-01,  6.4948e-01,  4.2186e-01],\n",
      "         [-2.6331e-01,  1.5349e-01,  1.8685e-01],\n",
      "         [ 1.5388e+00,  3.1621e-01,  5.8425e-01],\n",
      "         [-1.7524e+00,  5.4159e-01,  1.2413e-03]]],\n",
      "       grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.1016,  0.2241,  0.3748, -0.0201],\n",
      "         [-0.1251,  0.2089,  0.1151, -0.0490],\n",
      "         [-1.3994,  1.8268, -0.4094, -0.3962],\n",
      "         [-0.2272,  0.3102,  0.2040,  0.0128]],\n",
      "\n",
      "        [[-0.1937,  0.0097,  0.1460, -0.2349],\n",
      "         [-0.2469,  0.0065,  0.2549, -0.3620],\n",
      "         [ 0.1604,  0.1518,  0.0118,  0.2312],\n",
      "         [-0.4697, -0.0898,  0.3147, -0.6411]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.1968, 0.2726, 0.3170, 0.2136],\n",
      "         [0.2106, 0.2942, 0.2679, 0.2273],\n",
      "         [0.0316, 0.7969, 0.0852, 0.0863],\n",
      "         [0.1811, 0.3100, 0.2787, 0.2302]],\n",
      "\n",
      "        [[0.2179, 0.2670, 0.3060, 0.2091],\n",
      "         [0.2070, 0.2667, 0.3419, 0.1845],\n",
      "         [0.2547, 0.2525, 0.2195, 0.2733],\n",
      "         [0.1820, 0.2661, 0.3987, 0.1533]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1414,  0.2404, -0.7367],\n",
      "         [ 0.1992,  0.2386, -0.7779],\n",
      "         [ 0.5622,  0.6593, -1.3552],\n",
      "         [ 0.2002,  0.2678, -0.8071]],\n",
      "\n",
      "        [[ 0.2838,  0.0649, -0.6898],\n",
      "         [ 0.2928,  0.0741, -0.6652],\n",
      "         [ 0.2631,  0.0398, -0.7535],\n",
      "         [ 0.3059,  0.0887, -0.6283]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.2758,  0.3515, -0.9193],\n",
      "        [ 0.2864,  0.0669, -0.6842]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.3699, -0.2331],\n",
      "        [-0.2932, -0.2406]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.8026, -0.3615,  0.2940],\n",
      "         [-0.2589,  0.1600,  0.1821],\n",
      "         [-1.2900, -0.2035,  0.0161],\n",
      "         [-0.1904, -0.4520,  1.4480]],\n",
      "\n",
      "        [[ 0.1758,  0.3700,  0.2734],\n",
      "         [ 1.3389, -0.3842,  0.0627],\n",
      "         [-1.8866, -0.0593,  0.8383],\n",
      "         [ 0.2616,  0.4224, -0.7898]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 2.7515e-01,  1.3480e-01,  1.7276e-01,  3.8817e-01],\n",
      "         [ 5.5441e-01,  8.5523e-04, -1.0350e-01, -1.8931e-01],\n",
      "         [ 9.7966e-01, -9.0062e-02, -3.5254e-01, -5.1259e-01],\n",
      "         [ 6.5007e-01,  2.7013e-02, -7.9757e-02, -1.1282e-01]],\n",
      "\n",
      "        [[ 4.0952e-02,  3.3583e-01, -2.1186e-01,  2.1220e-01],\n",
      "         [ 9.0464e-02,  3.8168e-01, -2.4954e-02,  8.8093e-02],\n",
      "         [-2.4371e-02,  8.9022e-01, -1.1743e+00,  6.3390e-01],\n",
      "         [ 3.1098e-02,  4.0418e-01, -3.3532e-01,  2.3130e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2570, 0.2233, 0.2320, 0.2877],\n",
      "         [0.3894, 0.2239, 0.2017, 0.1851],\n",
      "         [0.5459, 0.1873, 0.1441, 0.1228],\n",
      "         [0.4025, 0.2158, 0.1940, 0.1877]],\n",
      "\n",
      "        [[0.2322, 0.3119, 0.1803, 0.2756],\n",
      "         [0.2366, 0.3166, 0.2108, 0.2360],\n",
      "         [0.1741, 0.4345, 0.0551, 0.3363],\n",
      "         [0.2290, 0.3325, 0.1587, 0.2797]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.0638,  0.1196, -0.3055],\n",
      "         [ 0.1070,  0.1886, -0.2976],\n",
      "         [ 0.1459,  0.2521, -0.2453],\n",
      "         [ 0.1086,  0.1915, -0.2875]],\n",
      "\n",
      "        [[ 0.2252,  0.2569, -0.5990],\n",
      "         [ 0.2076,  0.2339, -0.5805],\n",
      "         [ 0.2692,  0.3614, -0.5714],\n",
      "         [ 0.2322,  0.2726, -0.5922]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.1063,  0.1880, -0.2840],\n",
      "        [ 0.2336,  0.2812, -0.5858]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.1499, -0.2179],\n",
      "        [-0.2757, -0.1995]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 3 - Loss: 1.3430\n",
      "Current embedding: tensor([[[ 0.1305,  0.2469,  2.0551],\n",
      "         [ 0.7094,  0.9470, -1.9696],\n",
      "         [-2.0940, -1.5661, -0.1070],\n",
      "         [-0.0515,  1.1752, -0.1179]],\n",
      "\n",
      "        [[ 1.7958, -0.3688,  0.3014],\n",
      "         [-0.2572,  0.1626,  0.1819],\n",
      "         [-1.2968, -0.2109,  0.0235],\n",
      "         [-0.1937, -0.4594,  1.4554]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.0983,  0.2074,  0.3043, -0.0135],\n",
      "         [-0.1401,  0.2310,  0.0912, -0.0452],\n",
      "         [-1.4811,  1.9819, -0.5406, -0.3631],\n",
      "         [-0.2472,  0.3290,  0.1419,  0.0161]],\n",
      "\n",
      "        [[ 0.2642,  0.1352,  0.1786,  0.3975],\n",
      "         [ 0.5519, -0.0043, -0.1182, -0.2031],\n",
      "         [ 0.9904, -0.0969, -0.3795, -0.5414],\n",
      "         [ 0.6442,  0.0219, -0.0936, -0.1233]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2024, 0.2747, 0.3027, 0.2203],\n",
      "         [0.2079, 0.3014, 0.2621, 0.2286],\n",
      "         [0.0260, 0.8282, 0.0665, 0.0794],\n",
      "         [0.1800, 0.3202, 0.2656, 0.2342]],\n",
      "\n",
      "        [[0.2538, 0.2231, 0.2330, 0.2900],\n",
      "         [0.3914, 0.2244, 0.2003, 0.1839],\n",
      "         [0.5533, 0.1865, 0.1406, 0.1196],\n",
      "         [0.4034, 0.2165, 0.1929, 0.1872]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1797,  0.2546, -0.7747],\n",
      "         [ 0.2314,  0.2630, -0.8190],\n",
      "         [ 0.6273,  0.7123, -1.4321],\n",
      "         [ 0.2425,  0.2933, -0.8560]],\n",
      "\n",
      "        [[ 0.0539,  0.1106, -0.2951],\n",
      "         [ 0.0966,  0.1798, -0.2843],\n",
      "         [ 0.1330,  0.2417, -0.2264],\n",
      "         [ 0.0975,  0.1818, -0.2740]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.3202,  0.3808, -0.9705],\n",
      "        [ 0.0952,  0.1785, -0.2700]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.4178, -0.2007],\n",
      "        [-0.1430, -0.2207]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.5703,  0.6713,  0.4001],\n",
      "         [-0.2574,  0.1623,  0.1844],\n",
      "         [ 1.5599,  0.3380,  0.5626],\n",
      "         [-1.7313,  0.5634, -0.0206]],\n",
      "\n",
      "        [[ 0.1632,  0.3563,  0.2871],\n",
      "         [ 1.3272, -0.3978,  0.0763],\n",
      "         [-1.8966, -0.0730,  0.8520],\n",
      "         [ 0.2482,  0.4088, -0.7761]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.2031, -0.0134,  0.1655, -0.2664],\n",
      "         [-0.2513, -0.0097,  0.2779, -0.3859],\n",
      "         [ 0.1748,  0.1453, -0.0246,  0.2557],\n",
      "         [-0.4892, -0.1178,  0.3723, -0.7007]],\n",
      "\n",
      "        [[ 0.0356,  0.3329, -0.2455,  0.2127],\n",
      "         [ 0.0934,  0.3692, -0.0131,  0.0826],\n",
      "         [-0.0366,  0.9030, -1.2752,  0.6520],\n",
      "         [ 0.0274,  0.4144, -0.3798,  0.2423]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2177, 0.2632, 0.3148, 0.2044],\n",
      "         [0.2064, 0.2628, 0.3504, 0.1804],\n",
      "         [0.2581, 0.2506, 0.2114, 0.2799],\n",
      "         [0.1777, 0.2577, 0.4207, 0.1439]],\n",
      "\n",
      "        [[0.2328, 0.3134, 0.1758, 0.2779],\n",
      "         [0.2378, 0.3133, 0.2137, 0.2352],\n",
      "         [0.1712, 0.4382, 0.0496, 0.3409],\n",
      "         [0.2284, 0.3364, 0.1520, 0.2832]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3083,  0.0835, -0.7071],\n",
      "         [ 0.3137,  0.0891, -0.6792],\n",
      "         [ 0.2940,  0.0651, -0.7929],\n",
      "         [ 0.3237,  0.1000, -0.6283]],\n",
      "\n",
      "        [[ 0.2161,  0.2525, -0.5857],\n",
      "         [ 0.1960,  0.2246, -0.5682],\n",
      "         [ 0.2540,  0.3523, -0.5518],\n",
      "         [ 0.2226,  0.2688, -0.5772]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.3099,  0.0844, -0.7019],\n",
      "        [ 0.2222,  0.2746, -0.5707]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.3262, -0.2118],\n",
      "        [-0.2728, -0.1976]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 4 - Loss: 1.3073\n",
      "Current embedding: tensor([[[ 1.7819, -0.3841,  0.3168],\n",
      "         [-0.2564,  0.1649,  0.1841],\n",
      "         [-1.3106, -0.2264,  0.0389],\n",
      "         [-0.1955, -0.4748,  1.4709]],\n",
      "\n",
      "        [[-0.5636,  0.6787,  0.3927],\n",
      "         [-0.2564,  0.1649,  0.1841],\n",
      "         [ 1.5664,  0.3454,  0.5553],\n",
      "         [-1.7245,  0.5708, -0.0280]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.2424,  0.1357,  0.1891,  0.4156],\n",
      "         [ 0.5527, -0.0140, -0.1486, -0.2326],\n",
      "         [ 1.0184, -0.1100, -0.4353, -0.5995],\n",
      "         [ 0.6334,  0.0138, -0.1182, -0.1400]],\n",
      "\n",
      "        [[-0.2045, -0.0193,  0.1724, -0.2739],\n",
      "         [-0.2513, -0.0140,  0.2854, -0.3914],\n",
      "         [ 0.1799,  0.1436, -0.0390,  0.2655],\n",
      "         [-0.4932, -0.1250,  0.3931, -0.7170]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2478, 0.2227, 0.2349, 0.2946],\n",
      "         [0.3969, 0.2252, 0.1969, 0.1810],\n",
      "         [0.5696, 0.1843, 0.1331, 0.1130],\n",
      "         [0.4047, 0.2178, 0.1908, 0.1867]],\n",
      "\n",
      "        [[0.2177, 0.2619, 0.3173, 0.2031],\n",
      "         [0.2063, 0.2615, 0.3528, 0.1793],\n",
      "         [0.2593, 0.2500, 0.2083, 0.2824],\n",
      "         [0.1763, 0.2548, 0.4278, 0.1410]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.0329,  0.0916, -0.2732],\n",
      "         [ 0.0744,  0.1612, -0.2557],\n",
      "         [ 0.1048,  0.2190, -0.1853],\n",
      "         [ 0.0734,  0.1606, -0.2456]],\n",
      "\n",
      "        [[ 0.3143,  0.0874, -0.7113],\n",
      "         [ 0.3185,  0.0918, -0.6824],\n",
      "         [ 0.3027,  0.0718, -0.8053],\n",
      "         [ 0.3270,  0.1010, -0.6263]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.0714,  0.1581, -0.2399],\n",
      "        [ 0.3156,  0.0880, -0.7063]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.1281, -0.2266],\n",
      "        [-0.3328, -0.2063]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 0.1519,  0.3421,  0.3012],\n",
      "         [ 1.3184, -0.4120,  0.0903],\n",
      "         [-1.9014, -0.0871,  0.8662],\n",
      "         [ 0.2348,  0.3946, -0.7620]],\n",
      "\n",
      "        [[ 0.1107,  0.2675,  2.0344],\n",
      "         [ 0.7264,  0.9674, -1.9902],\n",
      "         [-2.1146, -1.5460, -0.1264],\n",
      "         [-0.0689,  1.1959, -0.1386]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0307,  0.3301, -0.2728,  0.2115],\n",
      "         [ 0.0961,  0.3542,  0.0049,  0.0733],\n",
      "         [-0.0475,  0.9173, -1.3599,  0.6648],\n",
      "         [ 0.0233,  0.4236, -0.4201,  0.2506]],\n",
      "\n",
      "        [[-0.0831,  0.1737,  0.2366, -0.0042],\n",
      "         [-0.1742,  0.2779,  0.0456, -0.0406],\n",
      "         [-1.5714,  2.1798, -0.7062, -0.3190],\n",
      "         [-0.2853,  0.3734,  0.0631,  0.0218]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2333, 0.3148, 0.1723, 0.2796],\n",
      "         [0.2390, 0.3093, 0.2181, 0.2336],\n",
      "         [0.1686, 0.4424, 0.0454, 0.3437],\n",
      "         [0.2278, 0.3400, 0.1462, 0.2860]],\n",
      "\n",
      "        [[0.2105, 0.2721, 0.2897, 0.2277],\n",
      "         [0.2016, 0.3168, 0.2512, 0.2304],\n",
      "         [0.0202, 0.8610, 0.0480, 0.0708],\n",
      "         [0.1752, 0.3385, 0.2482, 0.2382]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.2057,  0.2468, -0.5708],\n",
      "         [ 0.1838,  0.2145, -0.5552],\n",
      "         [ 0.2366,  0.3412, -0.5289],\n",
      "         [ 0.2115,  0.2634, -0.5602]],\n",
      "\n",
      "        [[ 0.2173,  0.2655, -0.8120],\n",
      "         [ 0.2771,  0.2973, -0.8795],\n",
      "         [ 0.7044,  0.7771, -1.5209],\n",
      "         [ 0.2981,  0.3294, -0.9242]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.2094,  0.2665, -0.5538],\n",
      "        [ 0.3742,  0.4173, -1.0342]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2685, -0.1964],\n",
      "        [-0.4683, -0.1696]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 5 - Loss: 1.2809\n",
      "Current embedding: tensor([[[ 0.1469,  0.3345,  0.3088],\n",
      "         [ 1.3164, -0.4196,  0.0978],\n",
      "         [-1.9007, -0.0947,  0.8739],\n",
      "         [ 0.2277,  0.3870, -0.7543]],\n",
      "\n",
      "        [[ 0.1036,  0.2746,  2.0272],\n",
      "         [ 0.7305,  0.9747, -1.9973],\n",
      "         [-2.1218, -1.5391, -0.1334],\n",
      "         [-0.0755,  1.2032, -0.1457]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 2.8133e-02,  3.3073e-01, -2.8855e-01,  2.1217e-01],\n",
      "         [ 9.7369e-02,  3.4665e-01,  1.4227e-02,  6.8313e-02],\n",
      "         [-5.2723e-02,  9.2861e-01, -1.4034e+00,  6.7231e-01],\n",
      "         [ 2.0564e-02,  4.3013e-01, -4.4467e-01,  2.5622e-01]],\n",
      "\n",
      "        [[-7.8723e-02,  1.6417e-01,  2.1957e-01, -2.0135e-03],\n",
      "         [-1.9050e-01,  2.9875e-01,  2.5528e-02, -3.8739e-02],\n",
      "         [-1.6013e+00,  2.2446e+00, -7.6045e-01, -3.0730e-01],\n",
      "         [-3.0215e-01,  3.9443e-01,  3.6891e-02,  2.3966e-02]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2334, 0.3159, 0.1701, 0.2806],\n",
      "         [0.2395, 0.3074, 0.2204, 0.2327],\n",
      "         [0.1669, 0.4453, 0.0432, 0.3446],\n",
      "         [0.2273, 0.3423, 0.1427, 0.2877]],\n",
      "\n",
      "        [[0.2127, 0.2711, 0.2866, 0.2296],\n",
      "         [0.1986, 0.3239, 0.2464, 0.2311],\n",
      "         [0.0186, 0.8704, 0.0431, 0.0678],\n",
      "         [0.1725, 0.3462, 0.2422, 0.2391]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1999,  0.2437, -0.5618],\n",
      "         [ 0.1771,  0.2090, -0.5475],\n",
      "         [ 0.2269,  0.3352, -0.5155],\n",
      "         [ 0.2052,  0.2605, -0.5500]],\n",
      "\n",
      "        [[ 0.2288,  0.2697, -0.8246],\n",
      "         [ 0.2953,  0.3111, -0.9038],\n",
      "         [ 0.7290,  0.7982, -1.5499],\n",
      "         [ 0.3185,  0.3440, -0.9503]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.2023,  0.2621, -0.5437],\n",
      "        [ 0.3929,  0.4307, -1.0572]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2657, -0.1959],\n",
      "        [-0.4870, -0.1579]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.5443,  0.7008,  0.3708],\n",
      "         [-0.2551,  0.1735,  0.1842],\n",
      "         [ 1.5843,  0.3675,  0.5338],\n",
      "         [-1.7050,  0.5928, -0.0500]],\n",
      "\n",
      "        [[ 1.7644, -0.4058,  0.3383],\n",
      "         [-0.2551,  0.1735,  0.1842],\n",
      "         [-1.3292, -0.2480,  0.0606],\n",
      "         [-0.1897, -0.4965,  1.4925]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.2104, -0.0366,  0.1954, -0.2977],\n",
      "         [-0.2531, -0.0276,  0.3099, -0.4100],\n",
      "         [ 0.1929,  0.1394, -0.0834,  0.2936],\n",
      "         [-0.5072, -0.1478,  0.4599, -0.7686]],\n",
      "\n",
      "        [[ 0.2058,  0.1348,  0.2070,  0.4457],\n",
      "         [ 0.5547, -0.0276, -0.1920, -0.2717],\n",
      "         [ 1.0634, -0.1280, -0.5178, -0.6759],\n",
      "         [ 0.6183,  0.0059, -0.1444, -0.1471]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2171, 0.2583, 0.3257, 0.1989],\n",
      "         [0.2056, 0.2576, 0.3610, 0.1758],\n",
      "         [0.2623, 0.2486, 0.1990, 0.2901],\n",
      "         [0.1714, 0.2456, 0.4510, 0.1320]],\n",
      "\n",
      "        [[0.2379, 0.2216, 0.2382, 0.3024],\n",
      "         [0.4048, 0.2261, 0.1919, 0.1772],\n",
      "         [0.5934, 0.1803, 0.1221, 0.1042],\n",
      "         [0.4043, 0.2191, 0.1886, 0.1880]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3305,  0.0974, -0.7218],\n",
      "         [ 0.3312,  0.0984, -0.6897],\n",
      "         [ 0.3295,  0.0926, -0.8433],\n",
      "         [ 0.3339,  0.1012, -0.6159]],\n",
      "\n",
      "        [[ 0.0018,  0.0630, -0.2401],\n",
      "         [ 0.0399,  0.1318, -0.2109],\n",
      "         [ 0.0591,  0.1817, -0.1190],\n",
      "         [ 0.0363,  0.1273, -0.2019]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.3313,  0.0974, -0.7177],\n",
      "        [ 0.0343,  0.1260, -0.1930]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.3537, -0.1882],\n",
      "        [-0.1027, -0.2381]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 6 - Loss: 1.2562\n",
      "Current embedding: tensor([[[ 1.7602, -0.4133,  0.3456],\n",
      "         [-0.2557,  0.1767,  0.1846],\n",
      "         [-1.3350, -0.2555,  0.0680],\n",
      "         [-0.1852, -0.5040,  1.4999]],\n",
      "\n",
      "        [[ 0.1390,  0.3186,  0.3246],\n",
      "         [ 1.3171, -0.4356,  0.1133],\n",
      "         [-1.8951, -0.1104,  0.8898],\n",
      "         [ 0.2135,  0.3711, -0.7385]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.1946,  0.1344,  0.2132,  0.4571],\n",
      "         [ 0.5568, -0.0327, -0.2071, -0.2850],\n",
      "         [ 1.0801, -0.1351, -0.5465, -0.7005],\n",
      "         [ 0.6148,  0.0039, -0.1509, -0.1453]],\n",
      "\n",
      "        [[ 0.0228,  0.3344, -0.3221,  0.2143],\n",
      "         [ 0.1000,  0.3316,  0.0340,  0.0570],\n",
      "         [-0.0628,  0.9573, -1.4907,  0.6875],\n",
      "         [ 0.0138,  0.4456, -0.5006,  0.2683]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2347, 0.2210, 0.2391, 0.3052],\n",
      "         [0.4079, 0.2263, 0.1900, 0.1758],\n",
      "         [0.6018, 0.1785, 0.1183, 0.1014],\n",
      "         [0.4040, 0.2193, 0.1878, 0.1889]],\n",
      "\n",
      "        [[0.2334, 0.3187, 0.1653, 0.2826],\n",
      "         [0.2407, 0.3034, 0.2253, 0.2306],\n",
      "         [0.1631, 0.4524, 0.0391, 0.3454],\n",
      "         [0.2258, 0.3478, 0.1350, 0.2913]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.0093,  0.0528, -0.2276],\n",
      "         [ 0.0274,  0.1213, -0.1940],\n",
      "         [ 0.0422,  0.1680, -0.0943],\n",
      "         [ 0.0230,  0.1154, -0.1858]],\n",
      "\n",
      "        [[ 0.1867,  0.2366, -0.5409],\n",
      "         [ 0.1626,  0.1972, -0.5297],\n",
      "         [ 0.2058,  0.3220, -0.4851],\n",
      "         [ 0.1912,  0.2540, -0.5267]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.0208,  0.1144, -0.1754],\n",
      "        [ 0.1866,  0.2524, -0.5206]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.0928, -0.2428],\n",
      "        [-0.2589, -0.1952]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 0.0820,  0.2961,  2.0056],\n",
      "         [ 0.7386,  0.9965, -2.0187],\n",
      "         [-2.1437, -1.5187, -0.1549],\n",
      "         [-0.0962,  1.2251, -0.1674]],\n",
      "\n",
      "        [[-0.5328,  0.7152,  0.3565],\n",
      "         [-0.2573,  0.1771,  0.1876],\n",
      "         [ 1.5935,  0.3820,  0.5199],\n",
      "         [-1.6932,  0.6071, -0.0644]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-6.5155e-02,  1.3841e-01,  1.8233e-01,  1.5955e-03],\n",
      "         [-2.5024e-01,  3.7255e-01, -4.9745e-02, -3.1389e-02],\n",
      "         [-1.6898e+00,  2.4311e+00, -9.2562e-01, -2.8329e-01],\n",
      "         [-3.6085e-01,  4.7059e-01, -4.3012e-02,  3.1400e-02]],\n",
      "\n",
      "        [[-2.1501e-01, -4.8623e-02,  2.1272e-01, -3.1452e-01],\n",
      "         [-2.5623e-01, -3.7990e-02,  3.2880e-01, -4.2590e-01],\n",
      "         [ 2.0026e-01,  1.3819e-01, -1.1291e-01,  3.1099e-01],\n",
      "         [-5.1766e-01, -1.6578e-01,  5.0746e-01, -8.0528e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2186, 0.2679, 0.2799, 0.2336],\n",
      "         [0.1876, 0.3497, 0.2292, 0.2335],\n",
      "         [0.0145, 0.8950, 0.0312, 0.0593],\n",
      "         [0.1626, 0.3734, 0.2234, 0.2407]],\n",
      "\n",
      "        [[0.2164, 0.2556, 0.3320, 0.1959],\n",
      "         [0.2048, 0.2547, 0.3676, 0.1728],\n",
      "         [0.2640, 0.2481, 0.1930, 0.2949],\n",
      "         [0.1678, 0.2386, 0.4678, 0.1259]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.2588,  0.2816, -0.8591],\n",
      "         [ 0.3562,  0.3576, -0.9856],\n",
      "         [ 0.8011,  0.8609, -1.6362],\n",
      "         [ 0.3832,  0.3926, -1.0350]],\n",
      "\n",
      "        [[ 0.3369,  0.1001, -0.7238],\n",
      "         [ 0.3353,  0.0989, -0.6890],\n",
      "         [ 0.3451,  0.1045, -0.8659],\n",
      "         [ 0.3328,  0.0962, -0.6021]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.4498,  0.4732, -1.1290],\n",
      "        [ 0.3375,  0.0999, -0.7202]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.5425, -0.1243],\n",
      "        [-0.3613, -0.1810]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 7 - Loss: 1.2296\n",
      "Current embedding: tensor([[[ 0.1348,  0.3027,  0.3403],\n",
      "         [ 1.3229, -0.4514,  0.1286],\n",
      "         [-1.8859, -0.1260,  0.9055],\n",
      "         [ 0.1997,  0.3552, -0.7228]],\n",
      "\n",
      "        [[-0.5272,  0.7229,  0.3488],\n",
      "         [-0.2589,  0.1800,  0.1879],\n",
      "         [ 1.5967,  0.3898,  0.5125],\n",
      "         [-1.6873,  0.6148, -0.0721]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0189,  0.3399, -0.3495,  0.2138],\n",
      "         [ 0.1036,  0.3175,  0.0589,  0.0421],\n",
      "         [-0.0686,  0.9923, -1.5636,  0.6973],\n",
      "         [ 0.0076,  0.4625, -0.5531,  0.2774]],\n",
      "\n",
      "        [[-0.2167, -0.0539,  0.2225, -0.3222],\n",
      "         [-0.2574, -0.0426,  0.3395, -0.4335],\n",
      "         [ 0.2014,  0.1363, -0.1273,  0.3165],\n",
      "         [-0.5207, -0.1730,  0.5331, -0.8211]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2333, 0.3217, 0.1614, 0.2836],\n",
      "         [0.2418, 0.2995, 0.2313, 0.2274],\n",
      "         [0.1596, 0.4612, 0.0358, 0.3434],\n",
      "         [0.2244, 0.3536, 0.1281, 0.2939]],\n",
      "\n",
      "        [[0.2161, 0.2543, 0.3352, 0.1944],\n",
      "         [0.2043, 0.2533, 0.3711, 0.1713],\n",
      "         [0.2646, 0.2480, 0.1905, 0.2969],\n",
      "         [0.1660, 0.2350, 0.4761, 0.1229]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1699,  0.2257, -0.5155],\n",
      "         [ 0.1450,  0.1822, -0.5079],\n",
      "         [ 0.1803,  0.3047, -0.4484],\n",
      "         [ 0.1732,  0.2436, -0.4985]],\n",
      "\n",
      "        [[ 0.3422,  0.1031, -0.7276],\n",
      "         [ 0.3395,  0.1008, -0.6915],\n",
      "         [ 0.3551,  0.1123, -0.8798],\n",
      "         [ 0.3342,  0.0951, -0.5981]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.1671,  0.2390, -0.4926],\n",
      "        [ 0.3428,  0.1028, -0.7243]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2445, -0.2005],\n",
      "        [-0.3695, -0.1739]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.7522, -0.4358,  0.3675],\n",
      "         [-0.2608,  0.1848,  0.1862],\n",
      "         [-1.3514, -0.2778,  0.0905],\n",
      "         [-0.1685, -0.5264,  1.5223]],\n",
      "\n",
      "        [[ 0.0676,  0.3101,  1.9914],\n",
      "         [ 0.7389,  1.0111, -2.0328],\n",
      "         [-2.1581, -1.5055, -0.1692],\n",
      "         [-0.1103,  1.2395, -0.1816]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 1.6271e-01,  1.3351e-01,  2.3261e-01,  4.9523e-01],\n",
      "         [ 5.7107e-01, -4.6588e-02, -2.5138e-01, -3.1753e-01],\n",
      "         [ 1.1419e+00, -1.5331e-01, -6.2779e-01, -7.5536e-01],\n",
      "         [ 6.1298e-01,  1.4946e-03, -1.6313e-01, -1.2334e-01]],\n",
      "\n",
      "        [[-5.5320e-02,  1.2155e-01,  1.6743e-01,  3.4940e-03],\n",
      "         [-2.9917e-01,  4.3509e-01, -1.0930e-01, -2.5595e-02],\n",
      "         [-1.7373e+00,  2.5435e+00, -1.0186e+00, -2.6880e-01],\n",
      "         [-4.0365e-01,  5.2953e-01, -9.5758e-02,  3.8139e-02]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2253, 0.2188, 0.2416, 0.3142],\n",
      "         [0.4184, 0.2256, 0.1838, 0.1721],\n",
      "         [0.6273, 0.1718, 0.1069, 0.0941],\n",
      "         [0.4030, 0.2186, 0.1854, 0.1930]],\n",
      "\n",
      "        [[0.2220, 0.2650, 0.2774, 0.2355],\n",
      "         [0.1783, 0.3716, 0.2156, 0.2344],\n",
      "         [0.0125, 0.9072, 0.0257, 0.0545],\n",
      "         [0.1548, 0.3937, 0.2107, 0.2408]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.0458,  0.0196, -0.1858],\n",
      "         [-0.0140,  0.0863, -0.1381],\n",
      "         [-0.0136,  0.1224, -0.0135],\n",
      "         [-0.0205,  0.0764, -0.1327]],\n",
      "\n",
      "        [[ 0.2793,  0.2920, -0.8846],\n",
      "         [ 0.4068,  0.3978, -1.0533],\n",
      "         [ 0.8494,  0.9030, -1.6952],\n",
      "         [ 0.4328,  0.4319, -1.1005]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.0235,  0.0762, -0.1175],\n",
      "        [ 0.4921,  0.5062, -1.1834]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.0555, -0.2627],\n",
      "        [-0.5904, -0.0930]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 8 - Loss: 1.1927\n",
      "Current embedding: tensor([[[ 0.0597,  0.3177,  1.9837],\n",
      "         [ 0.7368,  1.0190, -2.0404],\n",
      "         [-2.1659, -1.4985, -0.1770],\n",
      "         [-0.1181,  1.2473, -0.1893]],\n",
      "\n",
      "        [[ 0.1347,  0.2869,  0.3559],\n",
      "         [ 1.3325, -0.4673,  0.1438],\n",
      "         [-1.8744, -0.1416,  0.9213],\n",
      "         [ 0.1864,  0.3394, -0.7070]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-5.1759e-02,  1.1565e-01,  1.6173e-01,  3.9026e-03],\n",
      "         [-3.2946e-01,  4.7354e-01, -1.4597e-01, -2.1867e-02],\n",
      "         [-1.7639e+00,  2.6012e+00, -1.0691e+00, -2.6332e-01],\n",
      "         [-4.2935e-01,  5.6559e-01, -1.2498e-01,  4.2530e-02]],\n",
      "\n",
      "        [[ 1.6034e-02,  3.4630e-01, -3.7403e-01,  2.1333e-01],\n",
      "         [ 1.0756e-01,  3.0522e-01,  8.4025e-02,  2.7042e-02],\n",
      "         [-7.0481e-02,  1.0296e+00, -1.6253e+00,  7.0670e-01],\n",
      "         [ 1.5736e-03,  4.8236e-01, -6.0946e-01,  2.8724e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2233, 0.2640, 0.2765, 0.2361],\n",
      "         [0.1726, 0.3853, 0.2074, 0.2348],\n",
      "         [0.0116, 0.9131, 0.0233, 0.0521],\n",
      "         [0.1501, 0.4059, 0.2035, 0.2406]],\n",
      "\n",
      "        [[0.2333, 0.3246, 0.1579, 0.2842],\n",
      "         [0.2428, 0.2959, 0.2372, 0.2241],\n",
      "         [0.1565, 0.4701, 0.0331, 0.3404],\n",
      "         [0.2227, 0.3601, 0.1209, 0.2963]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.2904,  0.2986, -0.8992],\n",
      "         [ 0.4369,  0.4220, -1.0935],\n",
      "         [ 0.8752,  0.9258, -1.7266],\n",
      "         [ 0.4613,  0.4551, -1.1383]],\n",
      "\n",
      "        [[ 0.1546,  0.2161, -0.4914],\n",
      "         [ 0.1296,  0.1692, -0.4881],\n",
      "         [ 0.1558,  0.2882, -0.4122],\n",
      "         [ 0.1566,  0.2346, -0.4710]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.5159,  0.5254, -1.2144],\n",
      "        [ 0.1492,  0.2270, -0.4657]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.6191, -0.0738],\n",
      "        [-0.2350, -0.2012]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.7522, -0.4508,  0.3817],\n",
      "         [-0.2660,  0.1892,  0.1878],\n",
      "         [-1.3613, -0.2925,  0.1051],\n",
      "         [-0.1559, -0.5412,  1.5370]],\n",
      "\n",
      "        [[-0.5116,  0.7461,  0.3258],\n",
      "         [-0.2660,  0.1892,  0.1878],\n",
      "         [ 1.6015,  0.4132,  0.4904],\n",
      "         [-1.6705,  0.6378, -0.0953]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.1445,  0.1326,  0.2457,  0.5232],\n",
      "         [ 0.5889, -0.0574, -0.2862, -0.3453],\n",
      "         [ 1.1936, -0.1668, -0.6868, -0.7950],\n",
      "         [ 0.6185,  0.0017, -0.1669, -0.1021]],\n",
      "\n",
      "        [[-0.2224, -0.0698,  0.2535, -0.3463],\n",
      "         [-0.2629, -0.0574,  0.3736, -0.4600],\n",
      "         [ 0.2020,  0.1306, -0.1686,  0.3295],\n",
      "         [-0.5303, -0.1957,  0.6121, -0.8704]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2195, 0.2169, 0.2429, 0.3206],\n",
      "         [0.4285, 0.2245, 0.1786, 0.1684],\n",
      "         [0.6469, 0.1659, 0.0987, 0.0885],\n",
      "         [0.4029, 0.2174, 0.1837, 0.1960]],\n",
      "\n",
      "        [[0.2147, 0.2501, 0.3455, 0.1897],\n",
      "         [0.2025, 0.2486, 0.3826, 0.1662],\n",
      "         [0.2662, 0.2478, 0.1837, 0.3023],\n",
      "         [0.1602, 0.2238, 0.5020, 0.1140]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.0708, -0.0031, -0.1567],\n",
      "         [-0.0427,  0.0627, -0.0974],\n",
      "         [-0.0533,  0.0908,  0.0455],\n",
      "         [-0.0503,  0.0500, -0.0955]],\n",
      "\n",
      "        [[ 0.3576,  0.1116, -0.7390],\n",
      "         [ 0.3514,  0.1060, -0.6985],\n",
      "         [ 0.3866,  0.1369, -0.9232],\n",
      "         [ 0.3366,  0.0903, -0.5847]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.0542,  0.0501, -0.0760],\n",
      "        [ 0.3581,  0.1112, -0.7364]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.0292, -0.2764],\n",
      "        [-0.3950, -0.1516]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 9 - Loss: 1.1617\n",
      "Current embedding: tensor([[[ 0.0431,  0.3334,  1.9677],\n",
      "         [ 0.7291,  1.0356, -2.0563],\n",
      "         [-2.1822, -1.4840, -0.1932],\n",
      "         [-0.1345,  1.2636, -0.2054]],\n",
      "\n",
      "        [[-0.5068,  0.7541,  0.3180],\n",
      "         [-0.2699,  0.1918,  0.1889],\n",
      "         [ 1.6006,  0.4212,  0.4829],\n",
      "         [-1.6651,  0.6457, -0.1032]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.0461,  0.1069,  0.1529,  0.0039],\n",
      "         [-0.3999,  0.5642, -0.2294, -0.0122],\n",
      "         [-1.8193,  2.7152, -1.1741, -0.2564],\n",
      "         [-0.4872,  0.6490, -0.1880,  0.0538]],\n",
      "\n",
      "        [[-0.2240, -0.0757,  0.2636, -0.3537],\n",
      "         [-0.2653, -0.0634,  0.3857, -0.4699],\n",
      "         [ 0.2005,  0.1288, -0.1801,  0.3307],\n",
      "         [-0.5326, -0.2049,  0.6371, -0.8850]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2254, 0.2626, 0.2750, 0.2370],\n",
      "         [0.1592, 0.4175, 0.1888, 0.2346],\n",
      "         [0.0099, 0.9239, 0.0189, 0.0473],\n",
      "         [0.1392, 0.4337, 0.1878, 0.2392]],\n",
      "\n",
      "        [[0.2143, 0.2485, 0.3490, 0.1882],\n",
      "         [0.2018, 0.2469, 0.3869, 0.1644],\n",
      "         [0.2664, 0.2480, 0.1821, 0.3035],\n",
      "         [0.1584, 0.2199, 0.5103, 0.1114]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3134,  0.3134, -0.9307],\n",
      "         [ 0.5054,  0.4786, -1.1852],\n",
      "         [ 0.9289,  0.9733, -1.7928],\n",
      "         [ 0.5247,  0.5079, -1.2227]],\n",
      "\n",
      "        [[ 0.3626,  0.1142, -0.7431],\n",
      "         [ 0.3551,  0.1075, -0.7006],\n",
      "         [ 0.3975,  0.1454, -0.9376],\n",
      "         [ 0.3369,  0.0882, -0.5804]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.5681,  0.5683, -1.2828],\n",
      "        [ 0.3630,  0.1138, -0.7405]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.6841, -0.0295],\n",
      "        [-0.4046, -0.1431]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 0.1403,  0.2640,  0.3785],\n",
      "         [ 1.3498, -0.4902,  0.1656],\n",
      "         [-1.8556, -0.1639,  0.9442],\n",
      "         [ 0.1680,  0.3164, -0.6843]],\n",
      "\n",
      "        [[ 1.7569, -0.4662,  0.3963],\n",
      "         [-0.2743,  0.1962,  0.1879],\n",
      "         [-1.3709, -0.3075,  0.1202],\n",
      "         [-0.1418, -0.5564,  1.5522]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 1.2188e-02,  3.5985e-01, -4.1539e-01,  2.1408e-01],\n",
      "         [ 1.1338e-01,  2.9365e-01,  1.1645e-01,  3.7281e-03],\n",
      "         [-6.9008e-02,  1.0900e+00, -1.7168e+00,  7.2260e-01],\n",
      "         [-8.9390e-03,  5.1810e-01, -7.1112e-01,  3.0544e-01]],\n",
      "\n",
      "        [[ 1.3337e-01,  1.3082e-01,  2.5760e-01,  5.5498e-01],\n",
      "         [ 6.0625e-01, -6.9510e-02, -3.2167e-01, -3.7539e-01],\n",
      "         [ 1.2422e+00, -1.8166e-01, -7.4290e-01, -8.3187e-01],\n",
      "         [ 6.2493e-01,  1.6575e-03, -1.6715e-01, -7.4928e-02]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2330, 0.3299, 0.1519, 0.2851],\n",
      "         [0.2441, 0.2923, 0.2448, 0.2187],\n",
      "         [0.1518, 0.4838, 0.0292, 0.3351],\n",
      "         [0.2194, 0.3716, 0.1087, 0.3004]],\n",
      "\n",
      "        [[0.2149, 0.2143, 0.2433, 0.3275],\n",
      "         [0.4388, 0.2233, 0.1735, 0.1644],\n",
      "         [0.6650, 0.1601, 0.0913, 0.0836],\n",
      "         [0.4023, 0.2157, 0.1822, 0.1998]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1306,  0.2012, -0.4532],\n",
      "         [ 0.1060,  0.1497, -0.4561],\n",
      "         [ 0.1179,  0.2624, -0.3556],\n",
      "         [ 0.1303,  0.2205, -0.4268]],\n",
      "\n",
      "        [[-0.0951, -0.0253, -0.1269],\n",
      "         [-0.0706,  0.0399, -0.0572],\n",
      "         [-0.0928,  0.0591,  0.1043],\n",
      "         [-0.0792,  0.0243, -0.0590]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.1212,  0.2085, -0.4230],\n",
      "        [-0.0844,  0.0245, -0.0347]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2220, -0.2001],\n",
      "        [-0.0032, -0.2897]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 10 - Loss: 1.1268\n",
      "Current embedding: tensor([[[ 0.0265,  0.3490,  1.9517],\n",
      "         [ 0.7188,  1.0522, -2.0723],\n",
      "         [-2.1984, -1.4698, -0.2092],\n",
      "         [-0.1511,  1.2799, -0.2216]],\n",
      "\n",
      "        [[ 1.7614, -0.4745,  0.4041],\n",
      "         [-0.2795,  0.1984,  0.1893],\n",
      "         [-1.3760, -0.3156,  0.1283],\n",
      "         [-0.1340, -0.5645,  1.5603]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-4.4131e-02,  1.0194e-01,  1.3860e-01,  3.4945e-03],\n",
      "         [-4.7721e-01,  6.6782e-01, -3.1190e-01,  1.1594e-04],\n",
      "         [-1.8706e+00,  2.8153e+00, -1.2705e+00, -2.5369e-01],\n",
      "         [-5.4976e-01,  7.4105e-01, -2.5172e-01,  6.7928e-02]],\n",
      "\n",
      "        [[ 1.2884e-01,  1.3063e-01,  2.6411e-01,  5.7313e-01],\n",
      "         [ 6.1995e-01, -7.6448e-02, -3.4122e-01, -3.8848e-01],\n",
      "         [ 1.2748e+00, -1.9082e-01, -7.7339e-01, -8.4512e-01],\n",
      "         [ 6.3509e-01,  2.5453e-04, -1.7047e-01, -5.8944e-02]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2269, 0.2626, 0.2724, 0.2380],\n",
      "         [0.1442, 0.4532, 0.1701, 0.2324],\n",
      "         [0.0086, 0.9324, 0.0157, 0.0433],\n",
      "         [0.1276, 0.4639, 0.1719, 0.2366]],\n",
      "\n",
      "        [[0.2125, 0.2129, 0.2433, 0.3314],\n",
      "         [0.4453, 0.2219, 0.1703, 0.1624],\n",
      "         [0.6757, 0.1560, 0.0871, 0.0811],\n",
      "         [0.4038, 0.2140, 0.1804, 0.2017]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3420,  0.3326, -0.9688],\n",
      "         [ 0.5831,  0.5458, -1.2890],\n",
      "         [ 0.9871,  1.0246, -1.8650],\n",
      "         [ 0.5963,  0.5692, -1.3175]],\n",
      "\n",
      "        [[-0.1101, -0.0387, -0.1082],\n",
      "         [-0.0880,  0.0258, -0.0317],\n",
      "         [-0.1166,  0.0402,  0.1400],\n",
      "         [-0.0969,  0.0089, -0.0350]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.6271,  0.6180, -1.3601],\n",
      "        [-0.1029,  0.0090, -0.0087]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.7645,  0.0275],\n",
      "        [ 0.0152, -0.3000]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 0.1480,  0.2488,  0.3934],\n",
      "         [ 1.3630, -0.5055,  0.1801],\n",
      "         [-1.8420, -0.1788,  0.9593],\n",
      "         [ 0.1562,  0.3011, -0.6691]],\n",
      "\n",
      "        [[-0.4935,  0.7779,  0.2943],\n",
      "         [-0.2852,  0.1985,  0.1926],\n",
      "         [ 1.5924,  0.4454,  0.4605],\n",
      "         [-1.6496,  0.6693, -0.1270]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0111,  0.3741, -0.4439,  0.2131],\n",
      "         [ 0.1188,  0.2917,  0.1381, -0.0135],\n",
      "         [-0.0641,  1.1448, -1.7838,  0.7309],\n",
      "         [-0.0137,  0.5466, -0.7771,  0.3165]],\n",
      "\n",
      "        [[-0.2301, -0.0969,  0.2978, -0.3793],\n",
      "         [-0.2750, -0.0843,  0.4265, -0.5054],\n",
      "         [ 0.1892,  0.1212, -0.2071,  0.3233],\n",
      "         [-0.5380, -0.2364,  0.7141, -0.9285]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2328, 0.3346, 0.1477, 0.2849],\n",
      "         [0.2449, 0.2910, 0.2496, 0.2145],\n",
      "         [0.1483, 0.4968, 0.0266, 0.3284],\n",
      "         [0.2170, 0.3800, 0.1011, 0.3019]],\n",
      "\n",
      "        [[0.2128, 0.2431, 0.3608, 0.1833],\n",
      "         [0.1992, 0.2410, 0.4017, 0.1582],\n",
      "         [0.2666, 0.2491, 0.1794, 0.3049],\n",
      "         [0.1532, 0.2072, 0.5359, 0.1037]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.1096,  0.1868, -0.4209],\n",
      "         [ 0.0858,  0.1329, -0.4285],\n",
      "         [ 0.0856,  0.2396, -0.3076],\n",
      "         [ 0.1072,  0.2062, -0.3905]],\n",
      "\n",
      "        [[ 0.3792,  0.1235, -0.7570],\n",
      "         [ 0.3676,  0.1129, -0.7079],\n",
      "         [ 0.4320,  0.1726, -0.9813],\n",
      "         [ 0.3390,  0.0828, -0.5702]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.0971,  0.1914, -0.3869],\n",
      "        [ 0.3795,  0.1229, -0.7541]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.2046, -0.2058],\n",
      "        [-0.4387, -0.1127]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 11 - Loss: 1.0787\n",
      "Current embedding: tensor([[[ 0.0097,  0.3644,  1.9357],\n",
      "         [ 0.7066,  1.0689, -2.0884],\n",
      "         [-2.2147, -1.4558, -0.2250],\n",
      "         [-0.1678,  1.2963, -0.2378]],\n",
      "\n",
      "        [[ 0.1535,  0.2405,  0.4016],\n",
      "         [ 1.3708, -0.5138,  0.1879],\n",
      "         [-1.8342, -0.1868,  0.9676],\n",
      "         [ 0.1498,  0.2928, -0.6609]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-4.7654e-02,  1.0548e-01,  1.1536e-01,  1.0128e-04],\n",
      "         [-5.5532e-01,  7.7659e-01, -3.8956e-01,  1.7408e-02],\n",
      "         [-1.9170e+00,  2.9101e+00, -1.3699e+00, -2.6114e-01],\n",
      "         [-6.1650e-01,  8.4329e-01, -3.1633e-01,  8.5252e-02]],\n",
      "\n",
      "        [[ 1.1491e-02,  3.8123e-01, -4.5456e-01,  2.1142e-01],\n",
      "         [ 1.2225e-01,  2.9195e-01,  1.4973e-01, -2.2326e-02],\n",
      "         [-5.9463e-02,  1.1738e+00, -1.8105e+00,  7.3250e-01],\n",
      "         [-1.5353e-02,  5.6158e-01, -8.0814e-01,  3.2090e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2277, 0.2654, 0.2680, 0.2389],\n",
      "         [0.1292, 0.4893, 0.1525, 0.2290],\n",
      "         [0.0075, 0.9400, 0.0130, 0.0394],\n",
      "         [0.1153, 0.4964, 0.1557, 0.2326]],\n",
      "\n",
      "        [[0.2328, 0.3369, 0.1461, 0.2843],\n",
      "         [0.2452, 0.2906, 0.2520, 0.2122],\n",
      "         [0.1468, 0.5038, 0.0255, 0.3240],\n",
      "         [0.2158, 0.3843, 0.0977, 0.3021]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3735,  0.3541, -1.0104],\n",
      "         [ 0.6625,  0.6165, -1.3946],\n",
      "         [ 1.0447,  1.0762, -1.9359],\n",
      "         [ 0.6716,  0.6355, -1.4166]],\n",
      "\n",
      "        [[ 0.0977,  0.1783, -0.4028],\n",
      "         [ 0.0746,  0.1236, -0.4130],\n",
      "         [ 0.0674,  0.2264, -0.2803],\n",
      "         [ 0.0941,  0.1976, -0.3699]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.6881,  0.6706, -1.4394],\n",
      "        [ 0.0834,  0.1815, -0.3665]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.8481,  0.0874],\n",
      "        [-0.1937, -0.2102]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.4853,  0.7936,  0.2787],\n",
      "         [-0.2973,  0.2029,  0.1946],\n",
      "         [ 1.5833,  0.4613,  0.4458],\n",
      "         [-1.6396,  0.6848, -0.1427]],\n",
      "\n",
      "        [[ 1.7785, -0.4993,  0.4276],\n",
      "         [-0.2973,  0.2029,  0.1946],\n",
      "         [-1.3913, -0.3397,  0.1525],\n",
      "         [-0.1099, -0.5890,  1.5849]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-2.3418e-01, -1.1137e-01,  3.2287e-01, -3.9743e-01],\n",
      "         [-2.8326e-01, -9.9345e-02,  4.5726e-01, -5.3353e-01],\n",
      "         [ 1.8065e-01,  1.1587e-01, -2.2241e-01,  3.1670e-01],\n",
      "         [-5.4177e-01, -2.5851e-01,  7.6803e-01, -9.5982e-01]],\n",
      "\n",
      "        [[ 1.1981e-01,  1.3050e-01,  2.8503e-01,  6.3401e-01],\n",
      "         [ 6.7226e-01, -9.9345e-02, -4.0548e-01, -4.3018e-01],\n",
      "         [ 1.3897e+00, -2.2064e-01, -8.6861e-01, -8.7883e-01],\n",
      "         [ 6.7927e-01, -3.9764e-03, -1.7940e-01,  2.0422e-04]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2116, 0.2393, 0.3694, 0.1797],\n",
      "         [0.1969, 0.2367, 0.4130, 0.1533],\n",
      "         [0.2666, 0.2499, 0.1781, 0.3054],\n",
      "         [0.1494, 0.1984, 0.5538, 0.0984]],\n",
      "\n",
      "        [[0.2056, 0.2079, 0.2426, 0.3439],\n",
      "         [0.4685, 0.2166, 0.1594, 0.1556],\n",
      "         [0.7103, 0.1419, 0.0742, 0.0735],\n",
      "         [0.4105, 0.2073, 0.1740, 0.2082]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.3879,  0.1276, -0.7635],\n",
      "         [ 0.3731,  0.1139, -0.7087],\n",
      "         [ 0.4538,  0.1898, -1.0089],\n",
      "         [ 0.3371,  0.0763, -0.5596]],\n",
      "\n",
      "        [[-0.1595, -0.0836, -0.0460],\n",
      "         [-0.1465, -0.0218,  0.0550],\n",
      "         [-0.1966, -0.0238,  0.2597],\n",
      "         [-0.1563, -0.0428,  0.0464]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.3879,  0.1269, -0.7602],\n",
      "        [-0.1647, -0.0430,  0.0788]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.4576, -0.0955],\n",
      "        [ 0.0820, -0.3396]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 12 - Loss: 1.0244\n",
      "Current embedding: tensor([[[-0.0072,  0.3797,  1.9197],\n",
      "         [ 0.6929,  1.0858, -2.1046],\n",
      "         [-2.2308, -1.4421, -0.2404],\n",
      "         [-0.1846,  1.3126, -0.2540]],\n",
      "\n",
      "        [[-0.4811,  0.8020,  0.2703],\n",
      "         [-0.3046,  0.2055,  0.1957],\n",
      "         [ 1.5769,  0.4699,  0.4378],\n",
      "         [-1.6341,  0.6932, -0.1511]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.0529,  0.1117,  0.0918, -0.0048],\n",
      "         [-0.6390,  0.8958, -0.4675,  0.0388],\n",
      "         [-1.9613,  2.9880, -1.4658, -0.2809],\n",
      "         [-0.6863,  0.9530, -0.3803,  0.1056]],\n",
      "\n",
      "        [[-0.2348, -0.1189,  0.3336, -0.4037],\n",
      "         [-0.2870, -0.1076,  0.4719, -0.5468],\n",
      "         [ 0.1754,  0.1132, -0.2277,  0.3110],\n",
      "         [-0.5411, -0.2702,  0.7908, -0.9701]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2281, 0.2689, 0.2636, 0.2393],\n",
      "         [0.1137, 0.5275, 0.1349, 0.2239],\n",
      "         [0.0067, 0.9463, 0.0110, 0.0360],\n",
      "         [0.1029, 0.5302, 0.1397, 0.2272]],\n",
      "\n",
      "        [[0.2113, 0.2373, 0.3730, 0.1785],\n",
      "         [0.1959, 0.2345, 0.4185, 0.1511],\n",
      "         [0.2664, 0.2504, 0.1780, 0.3051],\n",
      "         [0.1481, 0.1942, 0.5612, 0.0965]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.4067,  0.3778, -1.0543],\n",
      "         [ 0.7468,  0.6935, -1.5065],\n",
      "         [ 1.1022,  1.1279, -2.0069],\n",
      "         [ 0.7507,  0.7067, -1.5203]],\n",
      "\n",
      "        [[ 0.3931,  0.1303, -0.7687],\n",
      "         [ 0.3764,  0.1148, -0.7104],\n",
      "         [ 0.4653,  0.1990, -1.0231],\n",
      "         [ 0.3370,  0.0737, -0.5569]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.7516,  0.7265, -1.5220],\n",
      "        [ 0.3930,  0.1294, -0.7648]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.9399,  0.1544],\n",
      "        [-0.4687, -0.0857]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.7916, -0.5156,  0.4431],\n",
      "         [-0.3122,  0.2100,  0.1948],\n",
      "         [-1.4020, -0.3554,  0.1682],\n",
      "         [-0.0938, -0.6050,  1.6010]],\n",
      "\n",
      "        [[ 0.1727,  0.2156,  0.4261],\n",
      "         [ 1.3948, -0.5389,  0.2115],\n",
      "         [-1.8102, -0.2111,  0.9924],\n",
      "         [ 0.1306,  0.2679, -0.6360]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.1254,  0.1287,  0.2964,  0.6792],\n",
      "         [ 0.7026, -0.1163, -0.4481, -0.4610],\n",
      "         [ 1.4573, -0.2429, -0.9269, -0.8977],\n",
      "         [ 0.7111, -0.0088, -0.1833,  0.0464]],\n",
      "\n",
      "        [[ 0.0127,  0.4036, -0.4865,  0.2052],\n",
      "         [ 0.1342,  0.3006,  0.1834, -0.0478],\n",
      "         [-0.0458,  1.2555, -1.8846,  0.7305],\n",
      "         [-0.0211,  0.6082, -0.9071,  0.3344]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2029, 0.2035, 0.2407, 0.3529],\n",
      "         [0.4832, 0.2130, 0.1529, 0.1509],\n",
      "         [0.7301, 0.1334, 0.0673, 0.0693],\n",
      "         [0.4149, 0.2020, 0.1696, 0.2134]],\n",
      "\n",
      "        [[0.2327, 0.3440, 0.1412, 0.2821],\n",
      "         [0.2460, 0.2905, 0.2584, 0.2051],\n",
      "         [0.1427, 0.5244, 0.0227, 0.3102],\n",
      "         [0.2121, 0.3979, 0.0874, 0.3026]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.1929, -0.1140, -0.0026],\n",
      "         [-0.1853, -0.0536,  0.1117],\n",
      "         [-0.2501, -0.0672,  0.3378],\n",
      "         [-0.1962, -0.0778,  0.1013]],\n",
      "\n",
      "        [[ 0.0600,  0.1511, -0.3460],\n",
      "         [ 0.0389,  0.0944, -0.3629],\n",
      "         [ 0.0100,  0.1840, -0.1953],\n",
      "         [ 0.0522,  0.1698, -0.3054]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.2061, -0.0781,  0.1371],\n",
      "        [ 0.0403,  0.1498, -0.3024]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.1298, -0.3695],\n",
      "        [-0.1576, -0.2256]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 13 - Loss: 0.9713\n",
      "Current embedding: tensor([[[-0.0242,  0.3948,  1.9038],\n",
      "         [ 0.6782,  1.1028, -2.1210],\n",
      "         [-2.2470, -1.4289, -0.2553],\n",
      "         [-0.2016,  1.3290, -0.2701]],\n",
      "\n",
      "        [[ 0.1800,  0.2070,  0.4346],\n",
      "         [ 1.4033, -0.5476,  0.2197],\n",
      "         [-1.8018, -0.2194,  1.0010],\n",
      "         [ 0.1237,  0.2593, -0.6275]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.0625,  0.1218,  0.0619, -0.0097],\n",
      "         [-0.7281,  1.0249, -0.5389,  0.0636],\n",
      "         [-2.0095,  3.0493, -1.5517, -0.3075],\n",
      "         [-0.7607,  1.0695, -0.4417,  0.1296]],\n",
      "\n",
      "        [[ 0.0138,  0.4134, -0.4968,  0.2018],\n",
      "         [ 0.1395,  0.3065,  0.1955, -0.0565],\n",
      "         [-0.0408,  1.2888, -1.9122,  0.7266],\n",
      "         [-0.0222,  0.6254, -0.9373,  0.3370]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2278, 0.2740, 0.2580, 0.2402],\n",
      "         [0.0982, 0.5666, 0.1186, 0.2167],\n",
      "         [0.0060, 0.9513, 0.0096, 0.0331],\n",
      "         [0.0905, 0.5644, 0.1245, 0.2205]],\n",
      "\n",
      "        [[0.2327, 0.3469, 0.1396, 0.2808],\n",
      "         [0.2462, 0.2910, 0.2604, 0.2024],\n",
      "         [0.1411, 0.5333, 0.0217, 0.3039],\n",
      "         [0.2108, 0.4028, 0.0844, 0.3019]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.4462,  0.4070, -1.1058],\n",
      "         [ 0.8379,  0.7792, -1.6271],\n",
      "         [ 1.1651,  1.1847, -2.0850],\n",
      "         [ 0.8366,  0.7857, -1.6326]],\n",
      "\n",
      "        [[ 0.0448,  0.1396, -0.3237],\n",
      "         [ 0.0247,  0.0826, -0.3431],\n",
      "         [-0.0132,  0.1664, -0.1613],\n",
      "         [ 0.0354,  0.1578, -0.2804]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.8215,  0.7891, -1.6126],\n",
      "        [ 0.0229,  0.1366, -0.2771]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-1.0495,  0.2372],\n",
      "        [-0.1406, -0.2345]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.8065, -0.5324,  0.4590],\n",
      "         [-0.3283,  0.2144,  0.1972],\n",
      "         [-1.4138, -0.3713,  0.1841],\n",
      "         [-0.0772, -0.6215,  1.6177]],\n",
      "\n",
      "        [[-0.4684,  0.8273,  0.2451],\n",
      "         [-0.3283,  0.2144,  0.1972],\n",
      "         [ 1.5557,  0.4956,  0.4140],\n",
      "         [-1.6173,  0.7181, -0.1765]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.1330,  0.1290,  0.3110,  0.7291],\n",
      "         [ 0.7478, -0.1369, -0.4976, -0.4884],\n",
      "         [ 1.5503, -0.2718, -0.9956, -0.9054],\n",
      "         [ 0.7627, -0.0179, -0.1962,  0.0978]],\n",
      "\n",
      "        [[-0.2384, -0.1454,  0.3657, -0.4246],\n",
      "         [-0.3016, -0.1369,  0.5155, -0.5913],\n",
      "         [ 0.1579,  0.1045, -0.2352,  0.2896],\n",
      "         [-0.5416, -0.3116,  0.8537, -1.0022]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.1998, 0.1990, 0.2387, 0.3626],\n",
      "         [0.5022, 0.2073, 0.1446, 0.1459],\n",
      "         [0.7542, 0.1219, 0.0591, 0.0647],\n",
      "         [0.4245, 0.1945, 0.1627, 0.2183]],\n",
      "\n",
      "        [[0.2102, 0.2307, 0.3846, 0.1745],\n",
      "         [0.1926, 0.2271, 0.4361, 0.1442],\n",
      "         [0.2657, 0.2519, 0.1793, 0.3031],\n",
      "         [0.1444, 0.1817, 0.5828, 0.0911]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.2327, -0.1508,  0.0490],\n",
      "         [-0.2335, -0.0937,  0.1830],\n",
      "         [-0.3148, -0.1205,  0.4323],\n",
      "         [-0.2457, -0.1212,  0.1717]],\n",
      "\n",
      "        [[ 0.4113,  0.1406, -0.7876],\n",
      "         [ 0.3883,  0.1192, -0.7179],\n",
      "         [ 0.5015,  0.2279, -1.0664],\n",
      "         [ 0.3397,  0.0684, -0.5541]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.2567, -0.1215,  0.2090],\n",
      "        [ 0.4102,  0.1390, -0.7815]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.1937, -0.4111],\n",
      "        [-0.5079, -0.0514]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 14 - Loss: 0.9088\n",
      "Current embedding: tensor([[[-0.4638,  0.8359,  0.2363],\n",
      "         [-0.3371,  0.2172,  0.1983],\n",
      "         [ 1.5477,  0.5045,  0.4058],\n",
      "         [-1.6111,  0.7267, -0.1852]],\n",
      "\n",
      "        [[-0.0413,  0.4096,  1.8880],\n",
      "         [ 0.6627,  1.1198, -2.1373],\n",
      "         [-2.2629, -1.4162, -0.2693],\n",
      "         [-0.2184,  1.3453, -0.2862]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.2386, -0.1547,  0.3754, -0.4297],\n",
      "         [-0.3069, -0.1478,  0.5306, -0.6072],\n",
      "         [ 0.1520,  0.1019, -0.2357,  0.2814],\n",
      "         [-0.5403, -0.3266,  0.8719, -1.0096]],\n",
      "\n",
      "        [[-0.0807,  0.1434,  0.0183, -0.0191],\n",
      "         [-0.8127,  1.1524, -0.5959,  0.0933],\n",
      "         [-2.0555,  3.0974, -1.6445, -0.3527],\n",
      "         [-0.8374,  1.1928, -0.5017,  0.1549]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2100, 0.2284, 0.3881, 0.1735],\n",
      "         [0.1914, 0.2245, 0.4423, 0.1418],\n",
      "         [0.2654, 0.2524, 0.1801, 0.3021],\n",
      "         [0.1435, 0.1777, 0.5891, 0.0898]],\n",
      "\n",
      "        [[0.2263, 0.2832, 0.2499, 0.2407],\n",
      "         [0.0844, 0.6021, 0.1048, 0.2088],\n",
      "         [0.0055, 0.9558, 0.0083, 0.0303],\n",
      "         [0.0787, 0.5991, 0.1100, 0.2122]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.4173,  0.1438, -0.7941],\n",
      "         [ 0.3916,  0.1199, -0.7195],\n",
      "         [ 0.5133,  0.2373, -1.0802],\n",
      "         [ 0.3403,  0.0663, -0.5537]],\n",
      "\n",
      "        [[ 0.4918,  0.4411, -1.1642],\n",
      "         [ 0.9264,  0.8637, -1.7438],\n",
      "         [ 1.2279,  1.2420, -2.1632],\n",
      "         [ 0.9241,  0.8677, -1.7465]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.4156,  0.1418, -0.7869],\n",
      "        [ 0.8926,  0.8536, -1.7044]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.5208, -0.0401],\n",
      "        [-1.1637,  0.3243]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.8226, -0.5495,  0.4753],\n",
      "         [-0.3462,  0.2218,  0.1972],\n",
      "         [-1.4268, -0.3874,  0.2002],\n",
      "         [-0.0600, -0.6383,  1.6348]],\n",
      "\n",
      "        [[ 0.2030,  0.1812,  0.4601],\n",
      "         [ 1.4294, -0.5737,  0.2444],\n",
      "         [-1.7765, -0.2446,  1.0268],\n",
      "         [ 0.1024,  0.2335, -0.6016]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.1564,  0.1269,  0.3215,  0.7860],\n",
      "         [ 0.7853, -0.1584, -0.5460, -0.5199],\n",
      "         [ 1.6306, -0.3023, -1.0592, -0.9140],\n",
      "         [ 0.8188, -0.0298, -0.2099,  0.1540]],\n",
      "\n",
      "        [[ 0.0182,  0.4458, -0.5271,  0.1912],\n",
      "         [ 0.1587,  0.3345,  0.2307, -0.0811],\n",
      "         [-0.0239,  1.3881, -1.9880,  0.7110],\n",
      "         [-0.0250,  0.6782, -1.0278,  0.3446]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.1989, 0.1931, 0.2346, 0.3733],\n",
      "         [0.5196, 0.2022, 0.1373, 0.1409],\n",
      "         [0.7745, 0.1121, 0.0526, 0.0608],\n",
      "         [0.4348, 0.1861, 0.1554, 0.2237]],\n",
      "\n",
      "        [[0.2324, 0.3565, 0.1347, 0.2764],\n",
      "         [0.2467, 0.2941, 0.2651, 0.1941],\n",
      "         [0.1364, 0.5599, 0.0191, 0.2845],\n",
      "         [0.2069, 0.4179, 0.0759, 0.2994]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.2746, -0.1895,  0.1054],\n",
      "         [-0.2813, -0.1337,  0.2525],\n",
      "         [-0.3788, -0.1738,  0.5239],\n",
      "         [-0.2967, -0.1659,  0.2448]],\n",
      "\n",
      "        [[-0.0037,  0.1026, -0.2530],\n",
      "         [-0.0209,  0.0448, -0.2791],\n",
      "         [-0.0866,  0.1097, -0.0543],\n",
      "         [-0.0178,  0.1190, -0.2021]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.3079, -0.1657,  0.2816],\n",
      "        [-0.0322,  0.0940, -0.1971]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.2624, -0.4573],\n",
      "        [-0.0831, -0.2666]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 15 - Loss: 0.8437\n",
      "Current embedding: tensor([[[-0.4535,  0.8540,  0.2181],\n",
      "         [-0.3561,  0.2245,  0.1982],\n",
      "         [ 1.5302,  0.5230,  0.3886],\n",
      "         [-1.5974,  0.7445, -0.2035]],\n",
      "\n",
      "        [[ 1.8316, -0.5587,  0.4841],\n",
      "         [-0.3561,  0.2245,  0.1982],\n",
      "         [-1.4343, -0.3958,  0.2086],\n",
      "         [-0.0508, -0.6472,  1.6440]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.2359, -0.1732,  0.3901, -0.4329],\n",
      "         [-0.3159, -0.1708,  0.5566, -0.6345],\n",
      "         [ 0.1384,  0.0955, -0.2318,  0.2610],\n",
      "         [-0.5320, -0.3560,  0.8979, -1.0113]],\n",
      "\n",
      "        [[ 0.1706,  0.1266,  0.3276,  0.8173],\n",
      "         [ 0.8101, -0.1708, -0.5726, -0.5331],\n",
      "         [ 1.6818, -0.3208, -1.0952, -0.9127],\n",
      "         [ 0.8565, -0.0375, -0.2202,  0.1854]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2103, 0.2239, 0.3932, 0.1727],\n",
      "         [0.1895, 0.2191, 0.4535, 0.1378],\n",
      "         [0.2646, 0.2535, 0.1827, 0.2991],\n",
      "         [0.1431, 0.1706, 0.5977, 0.0886]],\n",
      "\n",
      "        [[0.1986, 0.1900, 0.2323, 0.3791],\n",
      "         [0.5300, 0.1987, 0.1330, 0.1383],\n",
      "         [0.7862, 0.1061, 0.0489, 0.0587],\n",
      "         [0.4423, 0.1809, 0.1507, 0.2261]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.4339,  0.1546, -0.8149],\n",
      "         [ 0.4024,  0.1252, -0.7294],\n",
      "         [ 0.5391,  0.2582, -1.1096],\n",
      "         [ 0.3475,  0.0676, -0.5635]],\n",
      "\n",
      "        [[-0.2987, -0.2119,  0.1378],\n",
      "         [-0.3096, -0.1576,  0.2937],\n",
      "         [-0.4157, -0.2049,  0.5767],\n",
      "         [-0.3269, -0.1924,  0.2889]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.4307,  0.1514, -0.8044],\n",
      "        [-0.3377, -0.1917,  0.3243]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.5549, -0.0114],\n",
      "        [ 0.3044, -0.4861]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.0656,  0.4304,  1.8656],\n",
      "         [ 0.6399,  1.1443, -2.1608],\n",
      "         [-2.2852, -1.3989, -0.2881],\n",
      "         [-0.2424,  1.3684, -0.3087]],\n",
      "\n",
      "        [[ 0.2187,  0.1642,  0.4769],\n",
      "         [ 1.4469, -0.5909,  0.2609],\n",
      "         [-1.7599, -0.2611,  1.0437],\n",
      "         [ 0.0877,  0.2167, -0.5847]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.1144,  0.1811, -0.0531, -0.0354],\n",
      "         [-0.9320,  1.3377, -0.6519,  0.1466],\n",
      "         [-2.1300,  3.1449, -1.7592, -0.4285],\n",
      "         [-0.9495,  1.3749, -0.5691,  0.2008]],\n",
      "\n",
      "        [[ 0.0234,  0.4735, -0.5442,  0.1819],\n",
      "         [ 0.1761,  0.3665,  0.2520, -0.0975],\n",
      "         [-0.0109,  1.4637, -2.0380,  0.6962],\n",
      "         [-0.0251,  0.7136, -1.0775,  0.3448]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2227, 0.2993, 0.2368, 0.2411],\n",
      "         [0.0669, 0.6477, 0.0886, 0.1968],\n",
      "         [0.0049, 0.9610, 0.0071, 0.0270],\n",
      "         [0.0631, 0.6451, 0.0923, 0.1994]],\n",
      "\n",
      "        [[0.2322, 0.3642, 0.1316, 0.2720],\n",
      "         [0.2470, 0.2988, 0.2664, 0.1878],\n",
      "         [0.1328, 0.5803, 0.0175, 0.2694],\n",
      "         [0.2045, 0.4281, 0.0714, 0.2960]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.5689,  0.5020, -1.2622],\n",
      "         [ 1.0587,  0.9928, -1.9168],\n",
      "         [ 1.3279,  1.3348, -2.2875],\n",
      "         [ 1.0568,  0.9950, -1.9182]],\n",
      "\n",
      "        [[-0.0411,  0.0728, -0.1994],\n",
      "         [-0.0561,  0.0154, -0.2291],\n",
      "         [-0.1434,  0.0643,  0.0272],\n",
      "         [-0.0585,  0.0874, -0.1442]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 1.0031,  0.9561, -1.8462],\n",
      "        [-0.0748,  0.0600, -0.1364]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-1.3612,  0.4802],\n",
      "        [-0.0333, -0.2973]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 16 - Loss: 0.7745\n",
      "Current embedding: tensor([[[-0.0738,  0.4372,  1.8582],\n",
      "         [ 0.6321,  1.1525, -2.1686],\n",
      "         [-2.2925, -1.3934, -0.2938],\n",
      "         [-0.2503,  1.3760, -0.3160]],\n",
      "\n",
      "        [[ 0.2273,  0.1551,  0.4860],\n",
      "         [ 1.4564, -0.6002,  0.2698],\n",
      "         [-1.7511, -0.2700,  1.0528],\n",
      "         [ 0.0793,  0.2077, -0.5756]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-1.3063e-01,  1.9998e-01, -8.1172e-02, -4.3414e-02],\n",
      "         [-9.6734e-01,  1.3954e+00, -6.6424e-01,  1.6664e-01],\n",
      "         [-2.1532e+00,  3.1526e+00, -1.8008e+00, -4.6295e-01],\n",
      "         [-9.8650e-01,  1.4367e+00, -5.8944e-01,  2.1649e-01]],\n",
      "\n",
      "        [[ 2.7342e-02,  4.9120e-01, -5.5334e-01,  1.7717e-01],\n",
      "         [ 1.8640e-01,  3.8592e-01,  2.6366e-01, -1.0521e-01],\n",
      "         [-1.8498e-03,  1.5099e+00, -2.0648e+00,  6.8787e-01],\n",
      "         [-2.4212e-02,  7.3459e-01, -1.1030e+00,  3.4415e-01]]],\n",
      "       grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2206, 0.3070, 0.2318, 0.2407],\n",
      "         [0.0622, 0.6604, 0.0842, 0.1933],\n",
      "         [0.0048, 0.9625, 0.0068, 0.0259],\n",
      "         [0.0585, 0.6598, 0.0870, 0.1947]],\n",
      "\n",
      "        [[0.2319, 0.3688, 0.1298, 0.2694],\n",
      "         [0.2470, 0.3016, 0.2669, 0.1845],\n",
      "         [0.1306, 0.5924, 0.0166, 0.2604],\n",
      "         [0.2032, 0.4340, 0.0691, 0.2937]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 5.9803e-01,  5.2616e-01, -1.2994e+00],\n",
      "         [ 1.1010e+00,  1.0340e+00, -1.9717e+00],\n",
      "         [ 1.3613e+00,  1.3660e+00, -2.3290e+00],\n",
      "         [ 1.1010e+00,  1.0379e+00, -1.9751e+00]],\n",
      "\n",
      "        [[-6.2542e-02,  5.5510e-02, -1.6869e-01],\n",
      "         [-7.6125e-02, -1.4410e-03, -2.0082e-01],\n",
      "         [-1.7612e-01,  3.7919e-02,  7.4354e-02],\n",
      "         [-8.1684e-02,  6.9050e-02, -1.1117e-01]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 1.0403,  0.9910, -1.8938],\n",
      "        [-0.0991,  0.0403, -0.1016]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-1.4305,  0.5356],\n",
      "        [-0.0028, -0.3169]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.8586, -0.5859,  0.5103],\n",
      "         [-0.3852,  0.2338,  0.1999],\n",
      "         [-1.4577, -0.4206,  0.2333],\n",
      "         [-0.0235, -0.6739,  1.6714]],\n",
      "\n",
      "        [[-0.4376,  0.8800,  0.1918],\n",
      "         [-0.3852,  0.2338,  0.1999],\n",
      "         [ 1.5041,  0.5497,  0.3637],\n",
      "         [-1.5763,  0.7702, -0.2299]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.2360,  0.1229,  0.3412,  0.9216],\n",
      "         [ 0.8915, -0.2092, -0.6535, -0.5684],\n",
      "         [ 1.8515, -0.3791, -1.2049, -0.8961],\n",
      "         [ 1.0026, -0.0659, -0.2600,  0.2909]],\n",
      "\n",
      "        [[-0.2319, -0.2021,  0.4148, -0.4388],\n",
      "         [-0.3342, -0.2092,  0.6030, -0.6867],\n",
      "         [ 0.1199,  0.0870, -0.2219,  0.2317],\n",
      "         [-0.5212, -0.4033,  0.9379, -1.0173]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2004, 0.1790, 0.2227, 0.3979],\n",
      "         [0.5624, 0.1871, 0.1200, 0.1306],\n",
      "         [0.8206, 0.0882, 0.0386, 0.0526],\n",
      "         [0.4723, 0.1622, 0.1336, 0.2318]],\n",
      "\n",
      "        [[0.2104, 0.2168, 0.4017, 0.1711],\n",
      "         [0.1856, 0.2103, 0.4737, 0.1304],\n",
      "         [0.2634, 0.2549, 0.1872, 0.2945],\n",
      "         [0.1421, 0.1599, 0.6114, 0.0865]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.3790, -0.2875,  0.2482],\n",
      "         [-0.4024, -0.2376,  0.4285],\n",
      "         [-0.5351, -0.3078,  0.7456],\n",
      "         [-0.4295, -0.2830,  0.4410]],\n",
      "\n",
      "        [[ 0.4550,  0.1672, -0.8405],\n",
      "         [ 0.4122,  0.1267, -0.7339],\n",
      "         [ 0.5748,  0.2871, -1.1497],\n",
      "         [ 0.3534,  0.0645, -0.5715]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.4365, -0.2790,  0.4658],\n",
      "        [ 0.4489,  0.1613, -0.8239]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.4524, -0.5904],\n",
      "        [-0.6000,  0.0278]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 17 - Loss: 0.7045\n",
      "Current embedding: tensor([[[ 0.2452,  0.1361,  0.5048],\n",
      "         [ 1.4764, -0.6197,  0.2886],\n",
      "         [-1.7330, -0.2886,  1.0718],\n",
      "         [ 0.0613,  0.1890, -0.5566]],\n",
      "\n",
      "        [[ 1.8680, -0.5950,  0.5193],\n",
      "         [-0.3951,  0.2376,  0.1997],\n",
      "         [-1.4661, -0.4287,  0.2414],\n",
      "         [-0.0143, -0.6828,  1.6807]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0380,  0.5346, -0.5719,  0.1679],\n",
      "         [ 0.2111,  0.4342,  0.2871, -0.1191],\n",
      "         [ 0.0214,  1.6164, -2.1188,  0.6701],\n",
      "         [-0.0204,  0.7814, -1.1528,  0.3413]],\n",
      "\n",
      "        [[ 0.2685,  0.1200,  0.3431,  0.9611],\n",
      "         [ 0.9194, -0.2221, -0.6801, -0.5793],\n",
      "         [ 1.9110, -0.3990, -1.2405, -0.8868],\n",
      "         [ 1.0630, -0.0773, -0.2760,  0.3308]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2312, 0.3799, 0.1256, 0.2633],\n",
      "         [0.2470, 0.3088, 0.2666, 0.1776],\n",
      "         [0.1256, 0.6192, 0.0148, 0.2404],\n",
      "         [0.2005, 0.4470, 0.0646, 0.2879]],\n",
      "\n",
      "        [[0.2025, 0.1746, 0.2182, 0.4048],\n",
      "         [0.5731, 0.1830, 0.1158, 0.1281],\n",
      "         [0.8313, 0.0825, 0.0356, 0.0507],\n",
      "         [0.4848, 0.1550, 0.1271, 0.2331]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.1105,  0.0163, -0.1000],\n",
      "         [-0.1206, -0.0389, -0.1376],\n",
      "         [-0.2492, -0.0219,  0.1796],\n",
      "         [-0.1332,  0.0274, -0.0381]],\n",
      "\n",
      "        [[-0.4091, -0.3161,  0.2907],\n",
      "         [-0.4356, -0.2667,  0.4762],\n",
      "         [-0.5772, -0.3448,  0.8044],\n",
      "         [-0.4680, -0.3169,  0.4986]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.1534, -0.0043, -0.0240],\n",
      "        [-0.4725, -0.3111,  0.5175]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.0693, -0.3648],\n",
      "        [ 0.5095, -0.6316]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.0978,  0.4570,  1.8366],\n",
      "         [ 0.6087,  1.1768, -2.1917],\n",
      "         [-2.3136, -1.3782, -0.3091],\n",
      "         [-0.2731,  1.3982, -0.3371]],\n",
      "\n",
      "        [[-0.4261,  0.8971,  0.1743],\n",
      "         [-0.4057,  0.2399,  0.2010],\n",
      "         [ 1.4864,  0.5672,  0.3473],\n",
      "         [-1.5613,  0.7870, -0.2473]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.1914,  0.2703, -0.1742, -0.0760],\n",
      "         [-1.0583,  1.5530, -0.6824,  0.2335],\n",
      "         [-2.2168,  3.1493, -1.9266, -0.5878],\n",
      "         [-1.0936,  1.6193, -0.6404,  0.2649]],\n",
      "\n",
      "        [[-0.2269, -0.2208,  0.4290, -0.4383],\n",
      "         [-0.3477, -0.2369,  0.6361, -0.7249],\n",
      "         [ 0.1070,  0.0810, -0.2106,  0.2093],\n",
      "         [-0.5102, -0.4343,  0.9583, -1.0132]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2116, 0.3357, 0.2152, 0.2375],\n",
      "         [0.0507, 0.6908, 0.0739, 0.1846],\n",
      "         [0.0045, 0.9664, 0.0060, 0.0230],\n",
      "         [0.0464, 0.6999, 0.0731, 0.1806]],\n",
      "\n",
      "        [[0.2109, 0.2122, 0.4063, 0.1707],\n",
      "         [0.1826, 0.2040, 0.4883, 0.1252],\n",
      "         [0.2625, 0.2557, 0.1910, 0.2907],\n",
      "         [0.1423, 0.1535, 0.6181, 0.0861]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.6951,  0.6104, -1.4235],\n",
      "         [ 1.2220,  1.1520, -2.1276],\n",
      "         [ 1.4615,  1.4605, -2.4537],\n",
      "         [ 1.2312,  1.1658, -2.1423]],\n",
      "\n",
      "        [[ 0.4692,  0.1755, -0.8585],\n",
      "         [ 0.4164,  0.1252, -0.7337],\n",
      "         [ 0.5976,  0.3054, -1.1743],\n",
      "         [ 0.3574,  0.0621, -0.5785]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 1.1525,  1.0972, -2.0368],\n",
      "        [ 0.4602,  0.1671, -0.8362]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-1.6504,  0.7139],\n",
      "        [-0.6299,  0.0538]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 18 - Loss: 0.6376\n",
      "Current embedding: tensor([[[ 1.8876, -0.6140,  0.5378],\n",
      "         [-0.4163,  0.2442,  0.1999],\n",
      "         [-1.4840, -0.4450,  0.2575],\n",
      "         [ 0.0047, -0.7014,  1.7000]],\n",
      "\n",
      "        [[ 0.2629,  0.1172,  0.5236],\n",
      "         [ 1.4964, -0.6391,  0.3076],\n",
      "         [-1.7154, -0.3071,  1.0906],\n",
      "         [ 0.0427,  0.1706, -0.5379]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.3465,  0.1129,  0.3441,  1.0461],\n",
      "         [ 0.9827, -0.2497, -0.7344, -0.5954],\n",
      "         [ 2.0446, -0.4426, -1.3140, -0.8569],\n",
      "         [ 1.2029, -0.1038, -0.3128,  0.4182]],\n",
      "\n",
      "        [[ 0.0524,  0.5863, -0.5875,  0.1579],\n",
      "         [ 0.2417,  0.5003,  0.3059, -0.1318],\n",
      "         [ 0.0490,  1.7305, -2.1658,  0.6508],\n",
      "         [-0.0139,  0.8305, -1.1936,  0.3352]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2082, 0.1649, 0.2077, 0.4192],\n",
      "         [0.5961, 0.1738, 0.1071, 0.1230],\n",
      "         [0.8526, 0.0709, 0.0297, 0.0468],\n",
      "         [0.5137, 0.1391, 0.1128, 0.2344]],\n",
      "\n",
      "        [[0.2302, 0.3926, 0.1214, 0.2558],\n",
      "         [0.2469, 0.3198, 0.2633, 0.1700],\n",
      "         [0.1204, 0.6468, 0.0131, 0.2197],\n",
      "         [0.1980, 0.4605, 0.0608, 0.2807]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.4763, -0.3800,  0.3858],\n",
      "         [-0.5091, -0.3317,  0.5811],\n",
      "         [-0.6681, -0.4256,  0.9297],\n",
      "         [-0.5541, -0.3931,  0.6272]],\n",
      "\n",
      "        [[-0.1640, -0.0282, -0.0243],\n",
      "         [-0.1709, -0.0812, -0.0653],\n",
      "         [-0.3292, -0.0889,  0.2929],\n",
      "         [-0.1899, -0.0199,  0.0408]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.5519, -0.3826,  0.6310],\n",
      "        [-0.2135, -0.0545,  0.0610]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.6401, -0.7274],\n",
      "        [ 0.1550, -0.4240]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[-0.4131,  0.9147,  0.1563],\n",
      "         [-0.4274,  0.2472,  0.2001],\n",
      "         [ 1.4678,  0.5853,  0.3303],\n",
      "         [-1.5449,  0.8042, -0.2653]],\n",
      "\n",
      "        [[-0.1126,  0.4692,  1.8234],\n",
      "         [ 0.5939,  1.1919, -2.2061],\n",
      "         [-2.3263, -1.3694, -0.3176],\n",
      "         [-0.2869,  1.4117, -0.3497]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[-0.2155, -0.2361,  0.4359, -0.4249],\n",
      "         [-0.3576, -0.2643,  0.6642, -0.7560],\n",
      "         [ 0.0928,  0.0734, -0.1952,  0.1834],\n",
      "         [-0.4886, -0.4597,  0.9648, -0.9875]],\n",
      "\n",
      "        [[-0.2364,  0.3205, -0.2338, -0.0998],\n",
      "         [-1.1048,  1.6433, -0.6744,  0.2837],\n",
      "         [-2.2549,  3.1300, -1.9913, -0.6722],\n",
      "         [-1.1561,  1.7315, -0.6543,  0.3025]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2124, 0.2080, 0.4073, 0.1722],\n",
      "         [0.1803, 0.1979, 0.5008, 0.1210],\n",
      "         [0.2614, 0.2564, 0.1960, 0.2862],\n",
      "         [0.1446, 0.1489, 0.6187, 0.0878]],\n",
      "\n",
      "        [[0.2043, 0.3566, 0.2049, 0.2342],\n",
      "         [0.0451, 0.7046, 0.0694, 0.1809],\n",
      "         [0.0044, 0.9682, 0.0058, 0.0216],\n",
      "         [0.0402, 0.7208, 0.0663, 0.1727]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[ 0.4885,  0.1890, -0.8856],\n",
      "         [ 0.4242,  0.1272, -0.7399],\n",
      "         [ 0.6212,  0.3248, -1.1993],\n",
      "         [ 0.3682,  0.0667, -0.5983]],\n",
      "\n",
      "        [[ 0.7643,  0.6738, -1.5123],\n",
      "         [ 1.2977,  1.2266, -2.2244],\n",
      "         [ 1.5289,  1.5255, -2.5375],\n",
      "         [ 1.3139,  1.2484, -2.2479]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[ 0.4755,  0.1769, -0.8557],\n",
      "        [ 1.2262,  1.1686, -2.1305]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[-0.6680,  0.0858],\n",
      "        [-1.8089,  0.8453]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 19 - Loss: 0.5628\n",
      "Current embedding: tensor([[[ 0.2801,  0.0984,  0.5424],\n",
      "         [ 1.5164, -0.6586,  0.3268],\n",
      "         [-1.6987, -0.3255,  1.1093],\n",
      "         [ 0.0236,  0.1524, -0.5193]],\n",
      "\n",
      "        [[-0.1202,  0.4753,  1.8169],\n",
      "         [ 0.5864,  1.1996, -2.2133],\n",
      "         [-2.3326, -1.3652, -0.3214],\n",
      "         [-0.2937,  1.4184, -0.3558]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0697,  0.6444, -0.6013,  0.1484],\n",
      "         [ 0.2776,  0.5846,  0.3176, -0.1423],\n",
      "         [ 0.0791,  1.8425, -2.2005,  0.6309],\n",
      "         [-0.0057,  0.8799, -1.2278,  0.3276]],\n",
      "\n",
      "        [[-0.2597,  0.3465, -0.2602, -0.1101],\n",
      "         [-1.1259,  1.6882, -0.6650,  0.3118],\n",
      "         [-2.2730,  3.1187, -2.0124, -0.7087],\n",
      "         [-1.1851,  1.7869, -0.6538,  0.3251]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2289, 0.4066, 0.1170, 0.2476],\n",
      "         [0.2465, 0.3350, 0.2565, 0.1620],\n",
      "         [0.1153, 0.6726, 0.0118, 0.2002],\n",
      "         [0.1955, 0.4740, 0.0576, 0.2729]],\n",
      "\n",
      "        [[0.2002, 0.3671, 0.2001, 0.2325],\n",
      "         [0.0426, 0.7105, 0.0675, 0.1794],\n",
      "         [0.0044, 0.9688, 0.0057, 0.0211],\n",
      "         [0.0374, 0.7299, 0.0636, 0.1692]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.2208, -0.0760,  0.0557],\n",
      "         [-0.2261, -0.1270,  0.0152],\n",
      "         [-0.4112, -0.1588,  0.4076],\n",
      "         [-0.2489, -0.0699,  0.1222]],\n",
      "\n",
      "        [[ 0.8005,  0.7081, -1.5592],\n",
      "         [ 1.3359,  1.2645, -2.2732],\n",
      "         [ 1.5636,  1.5594, -2.5808],\n",
      "         [ 1.3552,  1.2900, -2.3005]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.2767, -0.1079,  0.1502],\n",
      "        [ 1.2638,  1.2055, -2.1784]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.2511, -0.4925],\n",
      "        [-1.8939,  0.9165]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Current embedding: tensor([[[ 1.9155, -0.6407,  0.5642],\n",
      "         [-0.4485,  0.2565,  0.1961],\n",
      "         [-1.5101, -0.4671,  0.2794],\n",
      "         [ 0.0314, -0.7273,  1.7273]],\n",
      "\n",
      "        [[-0.3988,  0.9327,  0.1377],\n",
      "         [-0.4485,  0.2565,  0.1961],\n",
      "         [ 1.4485,  0.6038,  0.3129],\n",
      "         [-1.5273,  0.8219, -0.2839]]], grad_fn=<EmbeddingBackward0>) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.5057,  0.0898,  0.3273,  1.1800],\n",
      "         [ 1.0698, -0.2887, -0.8082, -0.6189],\n",
      "         [ 2.2365, -0.5055, -1.4091, -0.8007],\n",
      "         [ 1.4408, -0.1523, -0.3748,  0.5569]],\n",
      "\n",
      "        [[-0.1968, -0.2456,  0.4358, -0.3972],\n",
      "         [-0.3618, -0.2887,  0.6857, -0.7758],\n",
      "         [ 0.0774,  0.0635, -0.1760,  0.1549],\n",
      "         [-0.4548, -0.4752,  0.9579, -0.9377]]], grad_fn=<DivBackward0>) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2243, 0.1480, 0.1876, 0.4402],\n",
      "         [0.6271, 0.1612, 0.0959, 0.1159],\n",
      "         [0.8783, 0.0566, 0.0229, 0.0421],\n",
      "         [0.5620, 0.1143, 0.0915, 0.2322]],\n",
      "\n",
      "        [[0.2149, 0.2047, 0.4045, 0.1759],\n",
      "         [0.1790, 0.1925, 0.5102, 0.1183],\n",
      "         [0.2602, 0.2566, 0.2020, 0.2812],\n",
      "         [0.1492, 0.1462, 0.6126, 0.0920]]], grad_fn=<SoftmaxBackward0>) \n",
      "\n",
      "*V:  tensor([[[-0.5879, -0.4865,  0.5473],\n",
      "         [-0.6197, -0.4313,  0.7358],\n",
      "         [-0.8021, -0.5475,  1.1100],\n",
      "         [-0.6915, -0.5152,  0.8314]],\n",
      "\n",
      "        [[ 0.5142,  0.2092, -0.9232],\n",
      "         [ 0.4372,  0.1345, -0.7549],\n",
      "         [ 0.6461,  0.3457, -1.2255],\n",
      "         [ 0.3878,  0.0801, -0.6329]]], grad_fn=<UnsafeViewBackward0>) \n",
      "\n",
      "Mean:  tensor([[-0.6753, -0.4952,  0.8061],\n",
      "        [ 0.4963,  0.1924, -0.8841]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "FC:  tensor([[ 0.8585, -0.8922],\n",
      "        [-0.7170,  0.1259]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Epoch 20 - Loss: 0.4828\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Training Loop\n",
    "model = SelfAttentionSentimentClassifier(vocab_size, embed_dim=3) # Model initialization\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # gradient-based weight optimizer. Updates model weights based on gradients computed during backprop\n",
    "loss_fn = nn.CrossEntropyLoss() # Sending loss to optimizer to improve the model\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "       optimizer.zero_grad()\n",
    "       logits, _ = model(x)\n",
    "       loss = loss_fn(logits, y)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a67c0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current embedding: tensor([[[ 0.2962,  0.0798,  0.5609],\n",
      "         [ 1.5366, -0.6780,  0.3462],\n",
      "         [-1.6831, -0.3437,  1.1278],\n",
      "         [ 0.0040,  0.1347, -0.5010]]]) \n",
      "\n",
      "QKt / size of Q:  tensor([[[ 0.0897,  0.7094, -0.6152,  0.1403],\n",
      "         [ 0.3192,  0.6886,  0.3206, -0.1500],\n",
      "         [ 0.1112,  1.9510, -2.2230,  0.6106],\n",
      "         [ 0.0034,  0.9283, -1.2567,  0.3193]]]) \n",
      "\n",
      "SOFTmax:  tensor([[[0.2271, 0.4219, 0.1122, 0.2388],\n",
      "         [0.2455, 0.3552, 0.2458, 0.1535],\n",
      "         [0.1106, 0.6964, 0.0107, 0.1823],\n",
      "         [0.1932, 0.4871, 0.0548, 0.2649]]]) \n",
      "\n",
      "*V:  tensor([[[-0.2802, -0.1262,  0.1395],\n",
      "         [-0.2864, -0.1763,  0.1046],\n",
      "         [-0.4936, -0.2302,  0.5219],\n",
      "         [-0.3088, -0.1214,  0.2046]]]) \n",
      "\n",
      "Mean:  tensor([[-0.3423, -0.1635,  0.2426]]) \n",
      "\n",
      "FC:  tensor([[ 0.3566, -0.5694]]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 6: Visualize attention\n",
    "def show_attention(input_tensor, attn_weights):\n",
    "    idxs = input_tensor[0].tolist()\n",
    "    tokens = [idx2word.get(idx, \"\") for idx in idxs]\n",
    "    attn = attn_weights[0].detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\", annot=True)\n",
    "    plt.title(\"Self-Attention Weights\")\n",
    "    plt.show()\n",
    "\n",
    "# Pick one example and visualize\n",
    "x, y = dataset[0]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, attn = model(x.unsqueeze(0))  # add batch dim\n",
    "    show_attention(x.unsqueeze(0), attn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d143c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
